#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import logging
import os
import sys
import pandas as pd
import json
import time
import pytz
import re
import yfinance as yf
from datetime import datetime, timezone, timedelta
from functools import wraps
from tqdm import tqdm
import google.generativeai as genai
from google.generativeai import types

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s', # å¢åŠ æ—¥å¿—çº§åˆ«æ˜¾ç¤º
    datefmt='%Y-%m-%d %H:%M:%S'
)

# é…ç½®å‚æ•°
FANGTANG_KEY = os.environ.get("FANGTANG_KEY", "") # ä»ç¯å¢ƒå˜é‡è·å–æ–¹ç³– KEY
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "") # ä»ç¯å¢ƒå˜é‡è·å– Gemini API KEY
LATEST_REPORT_FILE = "ç ”æŠ¥æ•°æ®/æ…§åšç ”æŠ¥_æœ€æ–°æ•°æ®.csv" # æœ€æ–°ç ”æŠ¥æ•°æ®æ–‡ä»¶
FINANCIAL_NEWS_DIR = "è´¢ç»æ–°é—»æ•°æ®" # è´¢ç»æ–°é—»æ•°æ®ç›®å½•
CLS_NEWS_DIR = "è´¢è”ç¤¾/output/cls" # è´¢è”ç¤¾æ–°é—»æ•°æ®ç›®å½•
MARKET_DATA_DIR = "å›½é™…å¸‚åœºæ•°æ®" # å›½é™…å¸‚åœºæ•°æ®ç›®å½•
DAILY_REPORT_DIR = "æ¯æ—¥æŠ¥å‘Š" # æ¯æ—¥æŠ¥å‘Šè¾“å‡ºç›®å½•
ANALYSIS_RESULT_DIR = "åˆ†æç»“æœ" # Checkåˆ†æç»“æœç›®å½•
RAW_DATA_DIR = "è‚¡ç¥¨åŸå§‹æ•°æ®" # è‚¡ç¥¨åŸå§‹æ•°æ®ç›®å½•
GEMINI_PROMPT_DIR = "Geminiå‘é€å†…å®¹" # Geminiå‘é€å†…å®¹å­˜å‚¨ç›®å½•

# åˆ›å»ºæ•°æ®ç›®å½•
logging.info("æ­£åœ¨æ£€æŸ¥å¹¶åˆ›å»ºæ‰€éœ€çš„æ•°æ®ç›®å½•...")
os.makedirs(FINANCIAL_NEWS_DIR, exist_ok=True)
os.makedirs(MARKET_DATA_DIR, exist_ok=True)
os.makedirs(DAILY_REPORT_DIR, exist_ok=True)
os.makedirs(ANALYSIS_RESULT_DIR, exist_ok=True)
os.makedirs(RAW_DATA_DIR, exist_ok=True)
os.makedirs(GEMINI_PROMPT_DIR, exist_ok=True)
logging.info("æ•°æ®ç›®å½•æ£€æŸ¥å®Œæ¯•ã€‚")


# é…ç½® Gemini API
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    logging.info("Gemini API KEY å·²é…ç½®ã€‚")
else:
    logging.warning("æœªæ‰¾åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡ï¼ŒAI ç›¸å…³åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")

# æ–°å¢ï¼šè´¢è”ç¤¾æ–°é—»æ‘˜è¦ä¸“ç”¨Prompt
CLS_SUMMARY_PROMPT = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„é‡‘èä¿¡æ¯åˆ†æå¸ˆå’Œæ–°é—»ç¼–è¾‘ã€‚

# ä»»åŠ¡
ä½ çš„ä»»åŠ¡æ˜¯å¿«é€Ÿé˜…è¯»å¹¶æç‚¼ä»¥ä¸‹æä¾›çš„â€œè´¢è”ç¤¾â€å®æ—¶ç”µæŠ¥æ–°é—»å†…å®¹ã€‚ä½ éœ€è¦ä»å¤§é‡ã€å¿«é€Ÿæ»šåŠ¨çš„ä¿¡æ¯æµä¸­ï¼Œç²¾å‡†åœ°æŠ½å–å‡ºå¯¹Aè‚¡å’Œæ¸¯è‚¡å¸‚åœºå¯èƒ½äº§ç”Ÿ **å®è´¨æ€§å½±å“** çš„æ ¸å¿ƒä¿¡æ¯ã€‚

# æŒ‡å¯¼åŸåˆ™
1.  **èšç„¦å…³é”®**: å…³æ³¨å®è§‚ç»æµæ”¿ç­–ã€å›½é™…å…³ç³»é‡å¤§äº‹ä»¶ã€å…¨çƒé‡è¦å…¬å¸çš„æœ€æ–°æ¶ˆæ¯ã€é‡è¦äº§ä¸šåŠ¨å‘ï¼ˆå¦‚ä»·æ ¼å˜åŠ¨ã€æŠ€æœ¯çªç ´ï¼‰ã€å…³é”®å…¬å¸é‡å¤§å…¬å‘Šï¼ˆå¦‚ä¸šç»©é¢„å‘Šã€å¹¶è´­é‡ç»„ï¼‰ã€ä»¥åŠå¯èƒ½å½±å“å¸‚åœºæƒ…ç»ªçš„é‡å¤§äº‹ä»¶ã€‚
2.  **è¿‡æ»¤å™ªéŸ³**: å¿½ç•¥å¸¸è§„çš„å¸‚åœºæ³¢åŠ¨æè¿°ã€é‡å¤ä¿¡æ¯ã€æ— æ˜ç¡®å½±å“çš„ä¼ é—»ã€ä»¥åŠè¿‡äºç»†èŠ‚çš„æŠ€æœ¯åˆ†æã€‚
3.  **é«˜åº¦æ¦‚æ‹¬**: ä½¿ç”¨ç®€æ´ã€ç²¾ç‚¼çš„è¯­è¨€è¿›è¡Œæ€»ç»“ã€‚æ¯ä¸ªè¦ç‚¹éƒ½åº”ç›´å‡»æ ¸å¿ƒã€‚
4.  **è¾“å‡ºæ ¼å¼**: ä»¥æ— åºåˆ—è¡¨ï¼ˆbullet pointsï¼‰çš„å½¢å¼è¾“å‡ºæ€»ç»“ã€‚

# å¾…å¤„ç†çš„åŸå§‹æ–°é—»å†…å®¹
{cls_news_content}
"""


# ç»¼åˆåˆ†æå¸ˆè§’è‰²æè¿°
ANALYST_PROMPT = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½æ‹¥æœ‰ä¸°å¯Œç»éªŒçš„ä¸­å›½Aè‚¡å’Œæ¸¯è‚¡å¸‚åœºåŸºé‡‘ç»ç†å’Œé¦–å¸­ï¼ˆæŠ•èµ„ã€æŠ•æœºï¼‰ç­–ç•¥åˆ†æå¸ˆï¼Œå¯¹å†²åŸºé‡‘é‡åŒ–åˆ†æå¸ˆï¼Œå°¤å…¶æ“…é•¿ä»æµ·é‡ã€æ··æ‚çš„åˆ¸å•†ç ”æŠ¥ã€è´¢ç»æ–°é—»å’Œå®æ—¶èµ„è®¯ä¸­ï¼Œé€šè¿‡äº¤å‰éªŒè¯å’Œé€»è¾‘æ¨æ¼”ï¼ŒæŒ–æ˜å‡ºæ½œåœ¨çš„ï¼ˆæŠ•èµ„ã€æŠ•æœºã€å¥—åˆ©ï¼‰æœºä¼šã€‚

# èƒŒæ™¯
å½“å‰æ—¶é—´ä¸º{current_time}ã€‚ä½ æ­£åœ¨è¿›è¡ŒæŠ•ç ”çš„å‡†å¤‡å·¥ä½œã€‚ä½ çš„ä¿¡æ¯æºåŒ…æ‹¬ï¼š
1. è¿‘æœŸï¼ˆè¿‡å»ä¸€å‘¨ï¼‰å‘å¸ƒçš„Aè‚¡åˆ¸å•†ç ”ç©¶æŠ¥å‘Šã€‚
2. æœ€æ–°çš„å®è§‚ç»æµæ•°æ®å’Œè´¢ç»æ–°é—»ã€‚
3. è´¢è”ç¤¾ç”µæŠ¥ç­‰å®æ—¶å¸‚åœºèµ„è®¯çš„ **æ ¸å¿ƒæ‘˜è¦**ã€‚
4. å…¨çƒä¸»è¦è‚¡æŒ‡ã€å¤§å®—å•†å“ã€å¤–æ±‡å’Œåˆ©ç‡ç­‰å›½é™…å¸‚åœºæ•°æ®ï¼ŒåŒ…æ‹¬é»„é‡‘ã€åŸæ²¹ã€é“œç­‰å¤§å®—å•†å“ä»·æ ¼èµ°åŠ¿ï¼Œç¾å…ƒæŒ‡æ•°ï¼Œä¸­ç¾å›½å€ºæ”¶ç›Šç‡ï¼Œä»¥åŠAè‚¡å’Œæ¸¯è‚¡ä¸»è¦æŒ‡æ•°è¡¨ç°ã€‚

ä½ çŸ¥é“è¿™äº›ä¿¡æ¯å……æ»¡äº†"å™ªéŸ³"ï¼Œä¸”å¯èƒ½å­˜åœ¨æ»åæ€§ã€ç‰‡é¢æ€§ç”šè‡³é”™è¯¯ã€‚å› æ­¤ï¼Œä½ çš„æ ¸å¿ƒä»·å€¼åœ¨äº **å¿«é€Ÿè¿‡æ»¤ã€ç²¾å‡†æç‚¼ã€ç‹¬ç«‹æ€è€ƒå’Œæ·±åº¦ç”„åˆ«**ï¼Œè€Œéç®€å•å¤è¿°ã€‚

# ä»»åŠ¡
è¯·ä½ åŸºäºä¸‹é¢æä¾›çš„å‚è€ƒèµ„æ–™ï¼Œä¸¥æ ¼éµå¾ªä»¥ä¸‹æŠ•èµ„çš„åˆ†ææ¡†æ¶ï¼Œå…ˆåˆ¤æ–­ä¸­å›½aè‚¡å’Œæ¸¯è‚¡æ˜¯å¦æœ‰é‡å¤§ä¸‹è¡Œé£é™©ã€‚åœ¨æ²¡æœ‰é‡å¤§ä¸‹è¡Œé£é™©çš„æƒ…å†µä¸‹ï¼Œä¸ºæˆ‘æ„å»ºå¹¶è¯¦ç»†é˜è¿°ä¸€ä¸ªç”±8-10åªAè‚¡æˆ–è€…æ¸¯è‚¡ç»„æˆçš„è¿‘æœŸæˆ–ä¸­é•¿æœŸå¯ä»¥èµšé’±æŠ•èµ„ç»„åˆã€‚

**åˆ†ææ¡†æ¶ (è¯·ä¸¥æ ¼æŒ‰æ­¥éª¤æ‰§è¡Œ):**

1. **å¸‚åœºå…³é”®ä¿¡æ¯æ¢³ç†ä¸å®šè°ƒ (Market Intelligence Briefing & Tone Setting):**
* **æ­¤ä¸ºé¦–è¦æ­¥éª¤ã€‚** è¯·é¦–å…ˆä»æ‰€æœ‰å‚è€ƒèµ„æ–™ï¼ˆç ”æŠ¥ã€æ–°é—»ã€è´¢è”ç¤¾æ‘˜è¦ã€å›½é™…å¸‚åœºæ•°æ®ï¼‰ä¸­ï¼Œæç‚¼å‡ºå¯¹ä»Šæ—¥ä¹ƒè‡³è¿‘æœŸAè‚¡æˆ–æ¸¯è‚¡æŠ•èµ„æœ‰ **é‡å¤§å½±å“** çš„å…³é”®ä¿¡æ¯ã€‚
* å°†è¿™äº›ä¿¡æ¯åˆ†ç±»æ•´ç†ä¸ºä»¥ä¸‹ä¸‰éƒ¨åˆ†ï¼Œå¹¶ç®€è¦è¯„ä¼°å…¶æ½œåœ¨å½±å“ï¼ˆåˆ©å¥½[+]ã€åˆ©ç©º[-]ã€ä¸­æ€§æˆ–ä¸ç¡®å®š[~]ï¼‰ï¼Œå¹¶æ ‡æ˜è¯¥äº‹ä»¶çš„å½±å“ç­‰çº§ï¼ˆ1æ˜Ÿ-5æ˜Ÿï¼Œä»å¼±åˆ°å¼ºï¼‰ï¼š
* **å®è§‚ä¸æ”¿ç­–åŠ¨æ€:** å¦‚é‡è¦çš„ç»æµæ•°æ®å‘å¸ƒã€äº§ä¸šæ”¿ç­–ã€ç›‘ç®¡åŠ¨å‘ã€å›½é™…å…³ç³»ç­‰ã€‚ç‰¹åˆ«å…³æ³¨å›½é™…å¸‚åœºåˆ†æä¸­æä¾›çš„å…¨çƒå¸‚åœºæƒ…ç»ªæŒ‡æ ‡ï¼Œå¦‚VIXæŒ‡æ•°ã€ç¾å…ƒæŒ‡æ•°ç­‰ã€‚è¿˜éœ€è¦æ ¹æ®ä¸­å›½å¤®è¡Œæœ€æ–°è´§å¸æ”¿ç­–ä¸æ“ä½œã€å›½å€ºå¸‚åœºã€è´§å¸å¸‚åœºçš„æœ€æ–°æ•°æ®ï¼Œåˆ†æä¸­å›½å›½å†…èµ„é‡‘æµåŠ¨æ€§æƒ…å†µã€‚
* **äº§ä¸šä¸ç§‘æŠ€å‰æ²¿:** å¹¿æ³›å…³æ³¨å°½é‡å¤šçš„è¡Œä¸šçš„ï¼šçªå‘åˆ©å¥½æˆ–åˆ©ç©ºæ¶ˆæ¯ã€è¡Œä¸šæ–°æ”¿ç­–æ³•è§„ã€å…³é”®æŠ€æœ¯çªç ´ï¼ˆå¦‚èŠ¯ç‰‡ã€å›ºæ€ç”µæ± ã€AIæ¨¡å‹ã€åˆ›æ–°è¯ç­‰ï¼‰ã€äº§ä¸šé“¾ä»·æ ¼å¼‚åŠ¨ã€è¡Œä¸šé‡è¦ä¼šè®®ç»“è®ºç­‰ã€‚**ç‰¹åˆ«å…³æ³¨ï¼šé‡è¦å•†å“æœŸè´§ï¼ˆå¦‚åŸæ²¹ã€é“œã€é»„é‡‘ï¼‰åŠå·¥ä¸šåŸæ–™ï¼ˆå¦‚é”‚ã€ç¨€åœŸï¼‰ã€ä¸­ç¾å›½å€ºåˆ©ç‡çš„ä»·æ ¼è¶‹åŠ¿ï¼Œå¹¶åˆ†æå…¶å¯¹ä¸Šä¸‹æ¸¸äº§ä¸šé“¾ï¼ˆå¦‚é‡‡æ˜ã€å†¶ç‚¼ã€åˆ¶é€ ã€åŒ–å·¥ï¼‰çš„æˆæœ¬å’Œåˆ©æ¶¦ä¼ å¯¼æ•ˆåº”ã€‚**
* **ç„¦ç‚¹å…¬å¸ä¸å¸‚åœºå¼‚åŠ¨:** å¹¿æ³›å…³æ³¨å°½é‡å¤šçš„å…¬å¸çš„çªå‘æ–°é—»ï¼ˆåˆ©å¥½æˆ–åˆ©ç©ºï¼‰ï¼›å…³æ³¨åº¦é«˜çš„å…¬å¸çš„æœ€æ–°å‘å¸ƒä¿¡æ¯ï¼›é¾™å¤´å…¬å¸çš„é‡å¤§åˆåŒã€ä¸šç»©é¢„è­¦/é¢„å–œã€å¹¶è´­é‡ç»„ï¼›å¯¹å„è¡Œä¸šå½±å“è¾ƒå¤§çš„çªå‘æ”¿ç­–æˆ–ä¿¡æ¯ï¼›å¸‚åœºèµ„é‡‘å¯¹æ¿å—çš„åå¥½æ˜¯å¦æœ‰è½¬å˜ã€‚

2. **æ ¸å¿ƒæŠ•èµ„ä¸»é¢˜è¯†åˆ« (Theme Identification):**
* åŸºäº **ç¬¬ä¸€æ­¥æ¢³ç†å‡ºçš„å…³é”®ä¿¡æ¯** å’Œæ‰€æœ‰ç ”æŠ¥æ‘˜è¦ï¼šè¯†åˆ«å¹¶å½’çº³å‡ºå½“å‰å¸‚åœºå…³æ³¨åº¦ä¸ä¸€å®šæœ€é«˜ï¼Œä½†ä¸­é•¿æœŸé€»è¾‘æœ€é¡ºã€å‚¬åŒ–å‰‚æœ€å¼ºå¹¶ä¸”ä¸æ˜¯çŸ­æœŸæŠ•æœºåšå¼ˆçš„4-8ä¸ªæ ¸å¿ƒæŠ•èµ„ä¸»é¢˜æˆ–èµ›é“ã€‚
* åŸºäº **ç¬¬ä¸€æ­¥æ¢³ç†å‡ºçš„å…³é”®ä¿¡æ¯** ï¼Œæ•´ç†å‡ºæ˜æ˜¾åˆ©ç©ºçš„è¡Œä¸šæ¿å—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
* æ¯ä¸ªä¸»é¢˜éœ€ç”¨100å­—ä»¥å†…çš„ä¸€æ®µè¯é˜æ˜å…¶æ ¸å¿ƒé€»è¾‘ï¼ˆä¾‹å¦‚ï¼šAIç®—åŠ›éœ€æ±‚çˆ†å‘ï¼Œå¸¦åŠ¨ä¸Šæ¸¸ç¡¬ä»¶äº§ä¸šé“¾æ™¯æ°”åº¦æŒç»­æå‡ï¼‰ã€‚

3. **å¤šæºäº¤å‰éªŒè¯ä¸ä¸ªè‚¡ç­›é€‰ (Cross-Validation & Stock Screening):**
* åœ¨è¯†åˆ«å‡ºçš„æ¯ä¸ªæ ¸å¿ƒä¸»é¢˜ä¸‹ï¼Œç­›é€‰å‡ºè¢« **è‡³å°‘2å®¶æˆ–ä»¥ä¸Šä¸åŒåˆ¸å•†** åŒæ—¶ç»™äºˆ"ä¹°å…¥"ã€"å¢æŒ"æˆ–åŒç­‰æ­£é¢è¯„çº§çš„ä¸ªè‚¡ã€‚
* ç»“åˆç¬¬ä¸€æ­¥æ•´ç†çš„æœ€æ–°æ–°é—»ï¼Œå¯¹å€™é€‰å…¬å¸è¿›è¡ŒäºŒæ¬¡éªŒè¯ï¼Œå‰”é™¤å­˜åœ¨æ½œåœ¨é‡å¤§åˆ©ç©ºä¿¡æ¯çš„å…¬å¸ï¼Œå½¢æˆæœ€ç»ˆå€™é€‰æ± ã€‚

4. **ä¸ªè‚¡æ·±åº¦å‰–æ (Deep Dive Analysis):**
* ä»å€™é€‰æ± ä¸­ï¼ŒåŸºäºä»¥ä¸‹æ ‡å‡†æŒ‘é€‰æœ€ç»ˆå…¥é€‰ç»„åˆçš„ä¸ªè‚¡ï¼š
* **æˆé•¿é©±åŠ¨åŠ›æ¸…æ™°**: å…¬å¸çš„ä¸»è¥ä¸šåŠ¡å¢é•¿é€»è¾‘æ˜¯å¦å¼ºåŠ²ä¸”å¯æŒç»­ï¼Ÿ
* **ä¸šç»©å¯è§æ€§é«˜**: ç ”æŠ¥æˆ–æ–°é—»ä¸­æ˜¯å¦æåŠå…·ä½“çš„ä¸šç»©é¢„å‘Šã€è®¢å•åˆåŒã€æˆ–æ˜ç¡®çš„ä¸šç»©æ”¹å–„ä¿¡å·ï¼Ÿ
* **å‚¬åŒ–å‰‚æ—¶æ•ˆæ€§**: å…¬å¸æ˜¯å¦ä¸è¿‘æœŸçƒ­ç‚¹æ–°é—»æˆ–æ”¿ç­–ç›´æ¥æˆ–é—´æ¥ç›¸å…³ï¼Œå…·å¤‡çŸ­æœŸæˆ–è€…ä¸­é•¿æœŸå‚¬åŒ–æ•ˆåº”ï¼Ÿ

5. **æŠ•èµ„ç»„åˆæ„å»ºä¸é£é™©ç®¡ç† (Portfolio Construction & Risk Management):**
* æœ€ç»ˆæ„å»ºä¸€ä¸ªåŒ…å«8-10åªaè‚¡å’Œæ¸¯è‚¡çš„è‚¡ç¥¨çš„æŠ•èµ„ç»„åˆã€‚
* ç»„åˆå†…åº”é€‚å½“åˆ†æ•£ï¼Œè¦†ç›–ä½ è¯†åˆ«å‡ºçš„ä¸»è¦æ ¸å¿ƒä¸»é¢˜ï¼Œé¿å…åœ¨å•ä¸€èµ›é“ä¸Šè¿‡åº¦é›†ä¸­ã€‚
* ä¸ºæ¯åªå…¥é€‰çš„è‚¡ç¥¨ï¼Œæ˜ç¡®å…¶åœ¨ç»„åˆä¸­çš„å®šä½ï¼ˆä¾‹å¦‚ï¼š"æ ¸å¿ƒé…ç½®"ä»£è¡¨é€»è¾‘æœ€å¼ºã€ç¡®å®šæ€§é«˜ï¼›"å«æ˜Ÿé…ç½®"ä»£è¡¨å¼¹æ€§è¾ƒå¤§ã€å±äºåšå–æ›´é«˜æ”¶ç›Šçš„éƒ¨åˆ†ï¼‰ã€‚
* ä¸ºæ¯åªå…¥é€‰çš„è‚¡ç¥¨ï¼Œæ ‡æ˜æŠ•èµ„å‘¨æœŸï¼Œ"çŸ­æœŸåšå¼ˆ"è¿˜æ˜¯"ä¸­é•¿æœŸæŠ•èµ„"ã€‚

**è¾“å‡ºæ ¼å¼ (è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„å‘ˆç°):**

**ä¸€ã€ å¸‚åœºå…³é”®ä¿¡æ¯é€Ÿè§ˆ (Market Intelligence Dashboard)**
* **å®è§‚ä¸æ”¿ç­–åŠ¨æ€:**
* ï¼ˆä¿¡æ¯ç‚¹1ï¼‰[å½±å“è¯„ä¼°: +/-/~]
* ï¼ˆä¿¡æ¯ç‚¹2ï¼‰[å½±å“è¯„ä¼°: +/-/~]
* **äº§ä¸šä¸ç§‘æŠ€å‰æ²¿:**
* ï¼ˆä¿¡æ¯ç‚¹1ï¼‰[å½±å“è¯„ä¼°: +/-/~]
* ï¼ˆä¿¡æ¯ç‚¹2ï¼‰[å½±å“è¯„ä¼°: +/-/~]
* **ç„¦ç‚¹å…¬å¸ä¸å¸‚åœºå¼‚åŠ¨:**
* ï¼ˆä¿¡æ¯ç‚¹1ï¼‰[å½±å“è¯„ä¼°: +/-/~]
* ï¼ˆä¿¡æ¯ç‚¹2ï¼‰[å½±å“è¯„ä¼°: +/-/~]

**äºŒã€ å¸‚åœºæ ¸å¿ƒæ´å¯Ÿä¸æŠ•èµ„ç­–ç•¥**
* ï¼ˆåŸºäºç¬¬ä¸€éƒ¨åˆ†çš„ä¿¡æ¯ï¼Œç®€è¦æ€»ç»“ä½ å¯¹å½“å‰å¸‚åœºæƒ…ç»ªçš„åˆ¤æ–­ã€è¯†åˆ«å‡ºçš„ä¸»è¦æœºä¼šä¸é£é™©ï¼Œå¹¶é˜è¿°æœ¬æ¬¡æ„å»ºç»„åˆçš„æ ¸å¿ƒç­–ç•¥ã€‚ï¼‰

**ä¸‰ã€ ç²¾é€‰æ ¸å¿ƒæŠ•èµ„ä¸»é¢˜**
* **ä¸»é¢˜ä¸€ï¼š** [ä¸»é¢˜åç§°] - [æ ¸å¿ƒé€»è¾‘é˜è¿°]
* **ä¸»é¢˜äºŒï¼š** [ä¸»é¢˜åç§°] - [æ ¸å¿ƒé€»è¾‘é˜è¿°]
* **ä¸»é¢˜ä¸‰ï¼š** [ä¸»é¢˜åç§°] - [æ ¸å¿ƒé€»è¾‘é˜è¿°]
* **ä¸»é¢˜å››ï¼š** [ä¸»é¢˜åç§°] - [æ ¸å¿ƒé€»è¾‘é˜è¿°]
* **ä¸»é¢˜äº”ï¼š** [ä¸»é¢˜åç§°] - [æ ¸å¿ƒé€»è¾‘é˜è¿°]
* **ä¸»é¢˜å…­ï¼š** [ä¸»é¢˜åç§°] - [æ ¸å¿ƒé€»è¾‘é˜è¿°]

**å››ã€ é«˜æˆé•¿æ½œåŠ›æ¨¡æ‹ŸæŠ•èµ„ç»„åˆè¯¦æƒ…**
ï¼ˆè¯·ä½¿ç”¨è¡¨æ ¼å‘ˆç°ï¼‰
| è‚¡ç¥¨ä»£ç  | å…¬å¸åç§° | æ ¸å¿ƒæŠ•èµ„é€»è¾‘ (ä¸‰å¥è¯æ¦‚æ‹¬) | æˆé•¿é©±åŠ¨å› ç´ ä¸è¿‘æœŸå‚¬åŒ–å‰‚ | ä¸»è¦é£é™©æç¤º | ç»„åˆå†…å®šä½ | | æŠ•èµ„å‘¨æœŸ |
| :--- | :--- | :--- | :--- | :--- | :--- || :--- |
| | | | | | ||
| ... | ... | ... | ... | ... | ... || :--- |

**äº”ã€ æŠ•èµ„ç»„åˆé£é™©å£°æ˜**
* æœ¬æ¨¡æ‹ŸæŠ•èµ„ç»„åˆå®Œå…¨åŸºäºæ‰€æä¾›çš„å†å²ä¿¡æ¯æ„å»ºï¼Œä»…ä½œä¸ºæŠ•èµ„ç ”ç©¶å’Œåˆ†ææ€è·¯çš„å±•ç¤ºã€‚æ‰€æœ‰ä¿¡æ¯å’Œè§‚ç‚¹å‡æœ‰æ—¶æ•ˆæ€§ï¼Œä¸æ„æˆä»»ä½•å®é™…çš„æŠ•èµ„å»ºè®®ã€‚æŠ•èµ„è€…åœ¨åšå‡ºä»»ä½•æŠ•èµ„å†³ç­–å‰ï¼Œå¿…é¡»è¿›è¡Œç‹¬ç«‹çš„æ·±å…¥ç ”ç©¶å’Œé£é™©è¯„ä¼°ã€‚å¸‚åœºæœ‰é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…ã€‚

# å‚è€ƒèµ„æ–™
ï¼ˆæ­¤éƒ¨åˆ†ç”¨äºåç»­ç²˜è´´å…·ä½“çš„ä¿¡æ¯æºï¼‰

## 1. ç ”æŠ¥æ•°æ®
{reports_data}

## 2. è´¢ç»æ–°é—»æ±‡æ€»
{financial_news}

## 3. è´¢è”ç¤¾ç”µæŠ¥ (æ ¸å¿ƒæ‘˜è¦)
{cls_news}

## 4. å›½é™…å¸‚åœºåˆ†æ
{market_analysis}
"""

def get_china_time():
    """è·å–ä¸­å›½æ—¶é—´"""
    utc_now = datetime.now(timezone.utc)
    china_now = utc_now + timedelta(hours=8)
    return china_now

def send_to_fangtang(title, content, short):
    """å‘é€æ¶ˆæ¯åˆ°æ–¹ç³–"""
    if not FANGTANG_KEY:
        logging.warning("æœªè®¾ç½®æ–¹ç³– KEYï¼Œè·³è¿‡æ¨é€")
        return False
    try:
        url = f"https://sctapi.ftqq.com/{FANGTANG_KEY}.send"
        data = {"title": title, "desp": content, "short": short}
        logging.info(f"å‡†å¤‡å‘æ–¹ç³–å‘é€æ¨é€ï¼Œæ ‡é¢˜: {title}")
        response = requests.post(url, data=data, timeout=10)
        response.raise_for_status()
        result = response.json()
        if result.get("code") == 0:
            logging.info("æ–¹ç³–æ¨é€æˆåŠŸ")
            return True
        else:
            logging.error(f"æ–¹ç³–æ¨é€å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            return False
    except Exception as e:
        logging.error(f"æ–¹ç³–æ¨é€å¼‚å¸¸: {str(e)}")
        return False

def save_gemini_prompt_to_file(prompt_content, prompt_type="comprehensive_analysis"):
    """å°†å‘é€ç»™Geminiçš„å®Œæ•´å†…å®¹ä¿å­˜åˆ°MDæ–‡ä»¶ä¸­"""
    logging.info(f"å‡†å¤‡ä¿å­˜{prompt_type}çš„Geminiå‘é€å†…å®¹åˆ°æ–‡ä»¶...")
    try:
        china_time = get_china_time()
        date_str = china_time.strftime('%Y-%m-%d')
        time_str = china_time.strftime('%H%M%S')

        prompt_date_dir = os.path.join(GEMINI_PROMPT_DIR, date_str)
        os.makedirs(prompt_date_dir, exist_ok=True)
        logging.info(f"Geminiå‘é€å†…å®¹å°†ä¿å­˜åœ¨ç›®å½•: {prompt_date_dir}")

        file_name = f"gemini_prompt_{prompt_type}_{time_str}.md"
        file_path = os.path.join(prompt_date_dir, file_name)

        # æ·»åŠ æ–‡ä»¶å¤´éƒ¨ä¿¡æ¯
        header = f"""# Gemini å‘é€å†…å®¹è®°å½•

**å‘é€æ—¶é—´:** {china_time.strftime('%Y-%m-%d %H:%M:%S')} (ä¸­å›½æ—¶é—´)
**å†…å®¹ç±»å‹:** {prompt_type}
**å†…å®¹é•¿åº¦:** {len(prompt_content)} å­—ç¬¦

---

"""
        
        full_content = header + prompt_content

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(full_content)

        logging.info(f"Geminiå‘é€å†…å®¹å·²æˆåŠŸä¿å­˜åˆ°: {file_path}")
        return file_path
    except Exception as e:
        logging.error(f"ä¿å­˜Geminiå‘é€å†…å®¹åˆ°æ–‡ä»¶å¤±è´¥: {str(e)}")
        return None

def load_research_reports():
    """åŠ è½½æœ€æ–°çš„ç ”æŠ¥æ•°æ®"""
    logging.info(f"å°è¯•ä» {LATEST_REPORT_FILE} åŠ è½½ç ”æŠ¥æ•°æ®...")
    try:
        if os.path.exists(LATEST_REPORT_FILE):
            with open(LATEST_REPORT_FILE, 'r', encoding='utf-8-sig') as f:
                csv_content = f.read()
            logging.info(f"æˆåŠŸåŠ è½½ç ”æŠ¥æ•°æ®æ–‡ä»¶ï¼Œå†…å®¹é•¿åº¦: {len(csv_content)} å­—ç¬¦")
            return csv_content
        else:
            logging.warning(f"ç ”æŠ¥æ•°æ®æ–‡ä»¶ {LATEST_REPORT_FILE} ä¸å­˜åœ¨")
            return None
    except Exception as e:
        logging.error(f"åŠ è½½ç ”æŠ¥æ•°æ®å¤±è´¥: {str(e)}")
        return None

def load_financial_news():
    """åŠ è½½æœ€æ–°çš„è´¢ç»æ–°é—»æ•°æ®"""
    logging.info(f"å°è¯•ä» {FINANCIAL_NEWS_DIR} åŠ è½½è´¢ç»æ–°é—»æ•°æ®...")
    try:
        current_date = get_china_time()
        current_year = current_date.year
        current_month = current_date.month
        archive_filename = f"financial_news_archive-{current_year}-{current_month:02d}.csv"
        file_path = os.path.join(FINANCIAL_NEWS_DIR, archive_filename)
        logging.info(f"æ­£åœ¨æŸ¥æ‰¾å½“æœˆè´¢ç»æ–°é—»æ–‡ä»¶: {file_path}")

        if os.path.exists(file_path):
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            logging.info(f"æ‰¾åˆ°æ–°é—»æ–‡ä»¶ï¼Œå…± {len(df)} æ¡è®°å½•")
            if len(df) > 400:
                df = df.head(400)
                logging.info("æ–°é—»è®°å½•è¶…è¿‡400æ¡ï¼Œå·²æˆªå–æœ€æ–°çš„400æ¡")

            news_content = df.to_string(index=False)
            logging.info(f"æˆåŠŸåŠ è½½è´¢ç»æ–°é—»æ•°æ®ï¼Œå†…å®¹é•¿åº¦: {len(news_content)} å­—ç¬¦")
            return news_content
        else:
            logging.warning(f"è´¢ç»æ–°é—»æ•°æ®æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
            return "æš‚æ— è´¢ç»æ–°é—»æ•°æ®"
    except Exception as e:
        logging.error(f"åŠ è½½è´¢ç»æ–°é—»æ•°æ®å¤±è´¥: {str(e)}")
        return "åŠ è½½è´¢ç»æ–°é—»æ•°æ®å¤±è´¥"

def load_cls_news():
    """åŠ è½½å½“å‰å‘¨çš„å…¨éƒ¨æ–°é—»å’Œä¸Šä¸€å‘¨çš„é‡è¦æ–°é—»ï¼Œå¹¶é™åˆ¶æ€»è¡Œæ•°ä¸è¶…è¿‡3000è¡Œ"""
    logging.info(f"å°è¯•ä» {CLS_NEWS_DIR} åŠ è½½è´¢è”ç¤¾æ–°é—»æ•°æ®...")
    try:
        current_date = get_china_time()
        current_week_num = current_date.isocalendar()[1]
        current_year = current_date.year
        
        prev_week_date = current_date - timedelta(days=7)
        prev_week_num = prev_week_date.isocalendar()[1]
        prev_year = prev_week_date.year

        current_week_str = f"{current_year}-W{current_week_num:02d}"
        prev_week_str = f"{prev_year}-W{prev_week_num:02d}"

        current_file_path = os.path.join(CLS_NEWS_DIR, f"cls_{current_week_str}.md")
        prev_file_path = os.path.join(CLS_NEWS_DIR, f"cls_{prev_week_str}.md")
        
        logging.info(f"æŸ¥æ‰¾å½“å‰å‘¨æ–‡ä»¶: {current_file_path}")
        logging.info(f"æŸ¥æ‰¾ä¸Šä¸€å‘¨æ–‡ä»¶: {prev_file_path}")

        current_week_content = ""
        prev_week_important_content = ""

        # åŠ è½½å½“å‰å‘¨çš„å…¨éƒ¨æ–°é—»
        if os.path.exists(current_file_path):
            with open(current_file_path, 'r', encoding='utf-8') as f:
                current_week_content = f.read()
            logging.info(f"æˆåŠŸåŠ è½½å½“å‰å‘¨ ({current_week_str}) è´¢è”ç¤¾æ–°é—»ï¼Œå†…å®¹é•¿åº¦: {len(current_week_content)}")
        else:
            logging.warning(f"å½“å‰å‘¨ ({current_week_str}) è´¢è”ç¤¾æ–°é—»æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")

        # åŠ è½½ä¸Šä¸€å‘¨çš„é‡è¦æ–°é—»
        if os.path.exists(prev_file_path):
            with open(prev_file_path, 'r', encoding='utf-8') as f:
                prev_week_content = f.read()
                # æå–é‡è¦æ–°é—»ï¼ˆæ ‡çº¢éƒ¨åˆ†ï¼‰
                important_news = []
                for line in prev_week_content.split('\n'):
                    if "**ğŸ”´ é‡è¦ç”µæŠ¥**" in line or "**" in line:  # å‡è®¾é‡è¦æ–°é—»ä»¥ ** æ ‡è®°
                        important_news.append(line)
                prev_week_important_content = "\n".join(important_news)
            logging.info(f"æˆåŠŸåŠ è½½ä¸Šä¸€å‘¨ ({prev_week_str}) è´¢è”ç¤¾é‡è¦æ–°é—»ï¼Œå†…å®¹é•¿åº¦: {len(prev_week_important_content)}")
        else:
            logging.warning(f"ä¸Šä¸€å‘¨ ({prev_week_str}) è´¢è”ç¤¾æ–°é—»æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")

        combined_content = ""
        if current_week_content:
            combined_content += f"# å½“å‰å‘¨ ({current_week_str}) è´¢è”ç¤¾ç”µæŠ¥\n\n{current_week_content}\n\n"
        if prev_week_important_content:
            combined_content += f"# ä¸Šä¸€å‘¨ ({prev_week_str}) è´¢è”ç¤¾é‡è¦ç”µæŠ¥\n\n{prev_week_important_content}"

        if not combined_content:
            logging.warning("æœªæ‰¾åˆ°ä»»ä½•è´¢è”ç¤¾æ–°é—»æ•°æ®")
            return "æš‚æ— è´¢è”ç¤¾æ–°é—»æ•°æ®"
        
        # é™åˆ¶æ€»è¡Œæ•°ä¸è¶…è¿‡3200è¡Œ
        lines = combined_content.split('\n')
        if len(lines) > 3200:
            combined_content = '\n'.join(lines[:3200])
            logging.warning(f"è´¢è”ç¤¾æ–°é—»æ•°æ®è¶…è¿‡3200è¡Œï¼Œå·²æˆªå–å‰3200è¡Œã€‚åŸå§‹è¡Œæ•°: {len(lines)}")
        
        logging.info(f"æˆåŠŸåˆå¹¶è´¢è”ç¤¾æ–°é—»æ•°æ®ï¼Œæ€»é•¿åº¦: {len(combined_content)}")
        return combined_content
    except Exception as e:
        logging.error(f"åŠ è½½è´¢è”ç¤¾æ–°é—»æ•°æ®å¤±è´¥: {str(e)}")
        return "åŠ è½½è´¢è”ç¤¾æ–°é—»æ•°æ®å¤±è´¥"

def load_market_data():
    """åŠ è½½æœ€æ–°çš„å›½é™…å¸‚åœºæ•°æ®"""
    logging.info(f"å°è¯•ä» {MARKET_DATA_DIR} åŠ è½½å›½é™…å¸‚åœºæ•°æ®...")
    try:
        market_data_file = os.path.join(MARKET_DATA_DIR, "global_market_data_latest.json")
        market_analysis_file = os.path.join(MARKET_DATA_DIR, "market_analysis_" + get_china_time().strftime('%Y-%m-%d') + ".md")
        market_data = None
        market_analysis = None

        if os.path.exists(market_data_file):
            with open(market_data_file, 'r', encoding='utf-8') as f:
                market_data = json.load(f)
            logging.info(f"æˆåŠŸåŠ è½½å›½é™…å¸‚åœºæ•°æ®: {market_data_file}")
        else:
            logging.warning(f"å›½é™…å¸‚åœºæ•°æ®æ–‡ä»¶ {market_data_file} ä¸å­˜åœ¨")

        if os.path.exists(market_analysis_file):
            with open(market_analysis_file, 'r', encoding='utf-8') as f:
                market_analysis = f.read()
            logging.info(f"æˆåŠŸåŠ è½½ä»Šæ—¥å›½é™…å¸‚åœºåˆ†æ: {market_analysis_file}")
        else:
            logging.warning(f"ä»Šæ—¥å›½é™…å¸‚åœºåˆ†ææ–‡ä»¶ {market_analysis_file} ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾æœ€è¿‘çš„åˆ†ææ–‡ä»¶...")
            analysis_files = [f for f in os.listdir(MARKET_DATA_DIR) if f.startswith("market_analysis_") and f.endswith(".md")]
            if analysis_files:
                latest_file = sorted(analysis_files)[-1]
                latest_file_path = os.path.join(MARKET_DATA_DIR, latest_file)
                with open(latest_file_path, 'r', encoding='utf-8') as f:
                    market_analysis = f.read()
                logging.info(f"æˆåŠŸåŠ è½½æœ€è¿‘çš„å›½é™…å¸‚åœºåˆ†æ: {latest_file}")
            else:
                logging.warning("æœªæ‰¾åˆ°ä»»ä½•å›½é™…å¸‚åœºåˆ†ææ–‡ä»¶")
        
        return market_data, market_analysis
    except Exception as e:
        logging.error(f"åŠ è½½å›½é™…å¸‚åœºæ•°æ®å¤±è´¥: {e}")
        return None, None

def summarize_cls_news_with_ai(cls_news_content):
    """ä½¿ç”¨AIæ‘˜è¦è´¢è”ç¤¾æ–°é—»"""
    if not GEMINI_API_KEY:
        logging.warning("æœªè®¾ç½® Gemini API KEYï¼Œè·³è¿‡è´¢è”ç¤¾æ–°é—»æ‘˜è¦")
        return "è´¢è”ç¤¾æ–°é—»æ‘˜è¦åŠŸèƒ½æœªå¯ç”¨ï¼ˆç¼ºå°‘API KEYï¼‰"
    if not cls_news_content:
        logging.info("è´¢è”ç¤¾æ–°é—»å†…å®¹ä¸ºç©ºï¼Œæ— éœ€æ‘˜è¦")
        return "æ— è´¢è”ç¤¾æ–°é—»å¯ä¾›æ‘˜è¦"
    
    logging.info("å¼€å§‹ä½¿ç”¨ Gemini æ‘˜è¦è´¢è”ç¤¾æ–°é—»...")
    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
        prompt = CLS_SUMMARY_PROMPT.format(cls_news_content=cls_news_content)
        
        # ä¿å­˜å‘é€ç»™Geminiçš„æ‘˜è¦prompt
        save_gemini_prompt_to_file(prompt, "cls_summary")
        
        # å¢åŠ ç­‰å¾…ï¼Œé˜²æ­¢APIé¢‘ç‡è¿‡å¿«
        logging.info("ç­‰å¾…30ç§’ï¼Œé˜²æ­¢APIé¢‘ç‡è¿‡å¿«...")
        time.sleep(30)

        response = model.generate_content(prompt)
        
        if response and hasattr(response, 'text'):
            summary = response.text
            logging.info(f"æˆåŠŸç”Ÿæˆè´¢è”ç¤¾æ–°é—»æ‘˜è¦ï¼Œæ‘˜è¦é•¿åº¦: {len(summary)} å­—ç¬¦")
            return summary
        else:
            logging.error("æ‘˜è¦è´¢è”ç¤¾æ–°é—»å¤±è´¥: å“åº”æ ¼å¼å¼‚å¸¸æˆ–å†…å®¹ä¸ºç©ºã€‚")
            logging.debug(f"åŸå§‹å“åº”: {response}")
            return "æ‘˜è¦è´¢è”ç¤¾æ–°é—»æ—¶å‘ç”Ÿé”™è¯¯ï¼Œå°†ä½¿ç”¨åŸå§‹æ•°æ®ã€‚"
    except Exception as e:
        logging.error(f"æ‘˜è¦è´¢è”ç¤¾æ–°é—»æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        return f"æ‘˜è¦è´¢è”ç¤¾æ–°é—»æ—¶å‘ç”Ÿå¼‚å¸¸ï¼Œå°†ä½¿ç”¨åŸå§‹æ•°æ®ã€‚é”™è¯¯: {str(e)}"

def generate_comprehensive_analysis(reports_data, financial_news, cls_news_summary, market_analysis):
    """ä½¿ç”¨ Gemini æ¨¡å‹ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    if not GEMINI_API_KEY:
        logging.warning("æœªè®¾ç½® Gemini API KEYï¼Œè·³è¿‡ç”Ÿæˆåˆ†æ")
        return "æœªé…ç½® Gemini API KEYï¼Œæ— æ³•ç”Ÿæˆåˆ†æ"
    try:
        model = genai.GenerativeModel('gemini-2.5-pro')
        china_time = get_china_time()
        weekday_names = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"]
        weekday = weekday_names[china_time.weekday()]
        current_time = f"{china_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')} {weekday}"

        prompt = ANALYST_PROMPT.format(
            reports_data=reports_data,
            financial_news=financial_news,
            cls_news=cls_news_summary, # ä½¿ç”¨æ‘˜è¦åçš„æ–°é—»
            current_time=current_time,
            market_analysis=market_analysis
        )
        logging.info("å¼€å§‹ä½¿ç”¨ Gemini ç”Ÿæˆç»¼åˆåˆ†æ... Prompt æ€»é•¿åº¦: " + str(len(prompt)))
        
        # ä¿å­˜å‘é€ç»™Geminiçš„å®Œæ•´å†…å®¹
        save_gemini_prompt_to_file(prompt, "comprehensive_analysis")
        
        # å¢åŠ 60ç§’ç­‰å¾…ï¼Œé˜²æ­¢APIé¢‘ç‡è¿‡å¿«
        logging.info("ç­‰å¾…60ç§’ï¼Œé˜²æ­¢APIé¢‘ç‡è¿‡å¿«...")
        time.sleep(60)
        
        response = model.generate_content(prompt)
        
        if response and hasattr(response, 'text'):
            logging.info("æˆåŠŸç”Ÿæˆç»¼åˆåˆ†æï¼Œå†…å®¹é•¿åº¦: " + str(len(response.text)))
            return response.text
        else:
            logging.error("ç”Ÿæˆåˆ†æå¤±è´¥: å“åº”æ ¼å¼å¼‚å¸¸æˆ–å†…å®¹ä¸ºç©ºã€‚")
            logging.debug(f"åŸå§‹å“åº”: {response}")
            try:
                # å°è¯•ä» parts è·å–
                text = response.parts[0].text
                logging.info("ä» response.parts æˆåŠŸæå–æ–‡æœ¬ã€‚")
                return text
            except (IndexError, AttributeError) as e:
                logging.error(f"æ— æ³•ä» response.parts æå–æ–‡æœ¬: {e}")
                return "ç”Ÿæˆåˆ†æå¤±è´¥: å“åº”æ ¼å¼å¼‚å¸¸"

    except Exception as e:
        logging.error(f"ç”Ÿæˆåˆ†æå¤±è´¥: {str(e)}")
        return f"ç”Ÿæˆåˆ†æå¤±è´¥: {str(e)}"


def save_analysis_to_file(analysis_content):
    """å°†åˆ†ææŠ¥å‘Šä¿å­˜åˆ°æŒ‡å®šç›®å½•çš„ MD æ–‡ä»¶ä¸­"""
    logging.info("å‡†å¤‡ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶...")
    try:
        china_time = get_china_time()
        date_str = china_time.strftime('%Y-%m-%d')
        time_str = china_time.strftime('%H%M%S')

        report_date_dir = os.path.join(DAILY_REPORT_DIR, date_str)
        os.makedirs(report_date_dir, exist_ok=True)
        logging.info(f"æŠ¥å‘Šå°†ä¿å­˜åœ¨ç›®å½•: {report_date_dir}")

        file_name = f"analysis_report_{time_str}.md"
        file_path = os.path.join(report_date_dir, file_name)

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(analysis_content)

        logging.info(f"åˆ†ææŠ¥å‘Šå·²æˆåŠŸä¿å­˜åˆ°: {file_path}")
        return file_path
    except Exception as e:
        logging.error(f"ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶å¤±è´¥: {str(e)}")
        return None

def send_analysis_report(analysis):
    """å‘é€åˆ†ææŠ¥å‘Šåˆ°æ–¹ç³–"""
    if not FANGTANG_KEY:
        logging.warning("æœªè®¾ç½®æ–¹ç³– KEYï¼Œè·³è¿‡æ¨é€")
        return False
    try:
        china_time = get_china_time()
        time_str = china_time.strftime('%Y-%m-%d %H:%M')
        title = f"æŠ•èµ„æ•°æ®ç»¼åˆAIåˆ†æ - {time_str} (ä¸­å›½æ—¶é—´)"
        content = analysis
        short = "æŠ•èµ„æ•°æ®ç»¼åˆAIåˆ†æå·²ç”Ÿæˆ"
        return send_to_fangtang(title, content, short)
    except Exception as e:
        logging.error(f"å‘é€åˆ†ææŠ¥å‘Šå¤±è´¥: {str(e)}")
        return False

# === Check.py é›†æˆåŠŸèƒ½ ===

def retry(retries=3, delay=5, backoff=2):
    """ä¸€ä¸ªå¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•è£…é¥°å™¨ã€‚"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            _retries, _delay = retries, delay
            while _retries > 0:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    _retries -= 1
                    if _retries == 0:
                        logging.error(f"å‡½æ•° {func.__name__} åœ¨æ‰€æœ‰é‡è¯•åå¤±è´¥: {e}")
                        return None
                    logging.warning(f"å‡½æ•° {func.__name__} å¤±è´¥: {e}. å°†åœ¨ {_delay} ç§’åé‡è¯•... ({_retries} æ¬¡é‡è¯•å‰©ä½™)")
                    time.sleep(_delay)
                    _delay *= backoff
        return wrapper
    return decorator

def extract_stock_recommendations(report_path):
    """ä» Markdown æŠ¥å‘Šä¸­è§£æå¹¶æå–æ¨èçš„è‚¡ç¥¨åˆ—è¡¨ã€‚"""
    logging.info(f"æ­£åœ¨ä»æŠ¥å‘Šä¸­æå–è‚¡ç¥¨æ¨è: {report_path}...")
    try:
        with open(report_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        stocks = []
        found_table = False

        for line in lines:
            line = line.strip()
            if not line.startswith('|') or line.count('|') < 3:
                continue
            cells = [c.strip() for c in line.split('|')[1:-1]]
            if not cells:
                continue

            if 'è‚¡ç¥¨ä»£ç ' in cells[0] and 'å…¬å¸åç§°' in cells[1]:
                found_table = True
                continue
            if not found_table:
                continue

            # åªæå– 6 ä½æ•°å­—
            m = re.search(r'\d{6}', cells[0])
            if not m:
                continue
            code = m.group()
            name = cells[1]

            stocks.append({'code': code, 'name': name})
            logging.debug(f"æ‰¾åˆ°è‚¡ç¥¨: {code} - {name}")
        
        if not stocks:
            logging.error("åœ¨æŠ¥å‘Šä¸­æ‰¾ä¸åˆ°ä»»ä½•è‚¡ç¥¨æ¨èã€‚")
            return None
            
        logging.info(f"æˆåŠŸæå– {len(stocks)} æ”¯æ¨èè‚¡ç¥¨: {[s['code'] for s in stocks]}")
        return stocks
    except Exception as e:
        logging.error(f"æå–è‚¡ç¥¨æ¨èæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def get_yahoo_ticker_symbol(code):
    """ä¸ºé›…è™è´¢ç»è‡ªåŠ¨æ·»åŠ æ­£ç¡®çš„äº¤æ˜“æ‰€åç¼€ã€‚"""
    code_str = str(code).strip()
    if code_str.startswith(('60', '68')):
        return f"{code_str}.SS"
    elif code_str.startswith(('00', '30', '20')):
        return f"{code_str}.SZ"
    else:
        logging.warning(f"æ— æ³•ç¡®å®šè‚¡ç¥¨ä»£ç  {code_str} çš„äº¤æ˜“æ‰€ã€‚å°†é»˜è®¤å°è¯•ä¸Šäº¤æ‰€ (.SS)ã€‚")
        return f"{code_str}.SS"

@retry(retries=3, delay=10)
def fetch_stock_data_from_yahoo(stock_info):
    """ä»Yahoo Financeè·å–ä¸€æ”¯è‚¡ç¥¨çš„è‚¡ä»·å’Œè´¢åŠ¡æ•°æ®ï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ°ã€‚"""
    code = stock_info['code']
    name = stock_info['name']
    ticker_symbol = get_yahoo_ticker_symbol(code)
    logging.info(f"å¼€å§‹ä¸º {name}({code}) è·å–æ•°æ®ï¼Œä½¿ç”¨é›…è™ä»£ç : {ticker_symbol}")
    
    stock_yf = yf.Ticker(ticker_symbol)
    stock_data_package = {"code": code, "name": name}
    stock_raw_data_dir = os.path.join(RAW_DATA_DIR, code)
    os.makedirs(stock_raw_data_dir, exist_ok=True)

    # 1. è‚¡ä»·å†å²
    logging.info(f"  > è·å– {name}({code}) æœ€è¿‘6ä¸ªæœˆè‚¡ä»·...")
    hist_data = stock_yf.history(period="6mo")
    if hist_data.empty:
        logging.warning(f"æœªèƒ½è·å– {name}({code}) çš„è‚¡ä»·å†å²æ•°æ®ã€‚å°†è·³è¿‡æ­¤è‚¡ç¥¨ã€‚")
        return None
    hist_data.sort_index(ascending=False, inplace=True)
    hist_data.to_csv(os.path.join(stock_raw_data_dir, f"{code}_price_6mo.csv"))
    hist_data.index = hist_data.index.strftime('%Y-%m-%d')
    hist_data = hist_data.where(pd.notnull(hist_data), None)
    stock_data_package["price_data"] = hist_data.reset_index().to_dict('records')
    logging.info(f"  > æˆåŠŸè·å– {name}({code}) çš„è‚¡ä»·æ•°æ®ã€‚")

    # 2. è´¢åŠ¡æŠ¥è¡¨
    try:
        def format_financial_statement(df):
            df.columns = df.columns.strftime('%Y-%m-%d')
            transposed_df = df.transpose()
            transposed_df = transposed_df.where(pd.notnull(transposed_df), None)
            return transposed_df.reset_index().to_dict('records')
        
        logging.info(f"  > è·å– {name}({code}) è´¢åŠ¡æŠ¥è¡¨...")
        income_stmt = stock_yf.income_stmt.iloc[:, :4]
        balance_sheet = stock_yf.balance_sheet.iloc[:, :4]
        cash_flow = stock_yf.cashflow.iloc[:, :4]

        income_stmt.to_csv(os.path.join(stock_raw_data_dir, f"{code}_income_4y.csv"))
        balance_sheet.to_csv(os.path.join(stock_raw_data_dir, f"{code}_balance_4y.csv"))
        cash_flow.to_csv(os.path.join(stock_raw_data_dir, f"{code}_cashflow_4y.csv"))

        stock_data_package["income_statement"] = format_financial_statement(income_stmt)
        stock_data_package["balance_sheet"] = format_financial_statement(balance_sheet)
        stock_data_package["cash_flow"] = format_financial_statement(cash_flow)
        logging.info(f"  > æˆåŠŸè·å–å¹¶ä¿å­˜ {name}({code}) çš„è´¢åŠ¡æ•°æ®åˆ° '{stock_raw_data_dir}'")

    except Exception as e:
        logging.warning(f"è·å– {name}({code}) çš„è´¢åŠ¡æ•°æ®æ—¶å‡ºé”™ (å¯èƒ½æ˜¯æ•°æ®ä¸å®Œæ•´): {e}ã€‚è´¢åŠ¡æ•°æ®å°†ä¸ºç©ºã€‚")
        stock_data_package["income_statement"] = []
        stock_data_package["balance_sheet"] = []
        stock_data_package["cash_flow"] = []

    return stock_data_package

@retry(retries=5, delay=10, backoff=2)
def analyze_stocks_in_batch(all_stock_data):
    """ä½¿ç”¨ Gemini AI æ¨¡å‹æ‰¹é‡åˆ†æè‚¡ç¥¨ã€‚"""
    if not GEMINI_API_KEY:
        logging.warning("GEMINI_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡ AI åˆ†æã€‚")
        return {}
    if not all_stock_data:
        logging.warning("æ²¡æœ‰è‚¡ç¥¨æ•°æ®å¯ä¾›åˆ†æï¼Œè·³è¿‡ã€‚")
        return {}

    logging.info(f"å‡†å¤‡ä½¿ç”¨ Gemini AI æ‰¹é‡åˆ†æ {len(all_stock_data)} æ”¯è‚¡ç¥¨...")
    prompt_header = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œã€ä¸€ä¸ä¸è‹Ÿçš„Aè‚¡å’Œæ¸¯è‚¡è‚¡ç¥¨åˆ†æå¸ˆï¼Œæ“…é•¿é€šè¿‡è§£è¯»è‚¡ä»·èµ°åŠ¿å’Œè´¢åŠ¡æŠ¥è¡¨æ¥è¯†åˆ«é•¿æœŸä»·å€¼å’Œæ½œåœ¨é£é™©ã€‚

# ä»»åŠ¡
æ ¹æ®ä¸‹é¢æä¾›çš„ä¸€ç»„è‚¡ç¥¨åˆ—è¡¨åŠå…¶ä»Yahoo Financeè·å–çš„åŸå§‹æ•°æ®ï¼ˆè¿‘åŠå¹´è‚¡ä»·å†å²å’Œè¿‘å‡ å¹´å¹´åº¦è´¢åŠ¡æŠ¥è¡¨ï¼‰ï¼Œä½ å¿…é¡» **æ— ä¸€ä¾‹å¤–åœ°** å¯¹åˆ—è¡¨ä¸­çš„ **æ¯ä¸€æ”¯** è‚¡ç¥¨è¿›è¡Œç»¼åˆåˆ†æã€‚ç„¶åï¼Œæ ¹æ®ä»¥ä¸‹æ ¸å¿ƒæ ‡å‡†ï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥å°†å…¶ **æ’é™¤** åœ¨ä¸€ä¸ªè§‚å¯Ÿåˆ—è¡¨ä¸­ï¼š

1. **æ¶¨å¹…å·¨å¤§ï¼Œç‚’ä½œæ¥è¿‘æ³¡æ²«**: è‚¡ç¥¨ä»·æ ¼åœ¨çŸ­æœŸå†…ï¼ˆä¾‹å¦‚è¿‡å»åŠå¹´ï¼‰å·²ç»ç»å†äº†éå¸¸å·¨å¤§çš„æ¶¨å¹…ï¼Œå¹¶é€šè¿‡è€ƒè™‘æœªæ¥é¢„ä¼°å¸‚ç›ˆç‡ç­‰ä¼°å€¼ä½“ç³»ï¼Œæ¥ç¡®å®šä¼°å€¼æ˜¯å¦è¿‡é«˜ï¼Œæ˜¯å¦å­˜åœ¨æ³¡æ²«é£é™©ã€‚
2. **ä¸šç»©å‡ºç°è¿‡æ˜æ˜¾é—®é¢˜**: å…¬å¸è¿‘æœŸå†…çš„è´¢åŠ¡æŠ¥è¡¨å­˜åœ¨é£é™©ç‚¹ï¼šå¦‚æ”¶å…¥å’Œæ¯›åˆ©ç‡çš„å¤§å¹…ä¸‹æ»‘ï¼Œèµ„äº§è´Ÿå€ºæƒ…å†µå˜å·®ï¼Œç°é‡‘æµæ˜æ˜¾å‡å°‘ç­‰

# ç‰¹æ®Šæƒ…å†µå¤„ç†
- **æ•°æ®ä¸å®Œæ•´**: å¦‚æœæŸæ”¯è‚¡ç¥¨çš„å…³é”®è´¢åŠ¡æ•°æ®ï¼ˆå¦‚åˆ©æ¶¦è¡¨ã€ç°é‡‘æµé‡è¡¨ï¼‰ä¸¥é‡ç¼ºå¤±ï¼Œ**ä¸è¦å°†å…¶æ’é™¤**ã€‚åœ¨åˆ†æä¸­æ˜ç¡®æŒ‡å‡ºæ•°æ®ä¸å®Œæ•´ï¼Œå¹¶å»ºè®®ç”¨æˆ·è‡ªè¡Œæ ¸å®ã€‚
"""
    prompt_data_section = f"""
# å¾…åˆ†ææ•°æ®
è¿™æ˜¯å¾…åˆ†æçš„è‚¡ç¥¨åˆ—è¡¨åŠå…¶æ•°æ®ï¼š
```json
{json.dumps(all_stock_data, ensure_ascii=False, indent=2, default=str)}
```
"""
    prompt_footer = """
# åˆ†æä¸è¾“å‡ºè¦æ±‚
1. **ä¸¥æ ¼éµå®ˆ**: ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ’é™¤æ ‡å‡†å’Œç‰¹æ®Šæƒ…å†µå¤„ç†è§„åˆ™ï¼Œå¯¹è¾“å…¥æ•°æ®ä¸­çš„ **æ¯ä¸€æ”¯è‚¡ç¥¨** è¿›è¡Œåˆ†æå¹¶å½¢æˆä¸€ä¸ªJSONå¯¹è±¡ã€‚
2. **å®Œæ•´æ€§ä¿è¯**: è¿”å›çš„JSONæ•°ç»„ **å¿…é¡»** åŒ…å«ä¸è¾“å…¥æ•°æ®ä¸­ç›¸åŒæ•°é‡çš„å¯¹è±¡ã€‚**ç»å¯¹ä¸è¦é—æ¼ä»»ä½•ä¸€æ”¯è‚¡ç¥¨**ã€‚
3. **å¼ºåˆ¶æ ¼å¼**: æ•°ç»„ä¸­æ¯ä¸ªå¯¹è±¡çš„ç»“æ„ **å¿…é¡»** å¦‚ä¸‹æ‰€ç¤ºã€‚`code` å’Œ `name` å­—æ®µè‡³å…³é‡è¦ã€‚
```json
{
"code": "åŸå§‹è‚¡ç¥¨ä»£ç , ä¾‹å¦‚ '600519'",
"name": "å…¬å¸åç§°, ä¾‹å¦‚ 'è´µå·èŒ…å°'",
"should_exclude": boolean,
"reason": "ä»…åœ¨ should_exclude ä¸º true æ—¶å¡«å†™æ­¤å­—æ®µï¼Œä»'è¿‘æœŸæ˜æ˜¾ä¸‹è·Œèµ°åŠ¿'ã€'æ¶¨å¹…å·¨å¤§æ¥è¿‘æ³¡æ²«'ä¸­é€‰æ‹©ä¸€ä¸ªã€‚å¦‚æœshould_excludeä¸ºfalseï¼Œæ­¤å­—æ®µåº”ä¸ºç©ºå­—ç¬¦ä¸²æˆ–nullã€‚",
"analysis": "æä¾›ä¸€å¥è¯çš„ç®€æ˜æ‰¼è¦çš„åˆ†æï¼Œè§£é‡Šä½ åšå‡ºè¯¥åˆ¤æ–­çš„æ ¸å¿ƒä¾æ®ã€‚å¯¹äºæ•°æ®ä¸å®Œæ•´çš„è‚¡ç¥¨ï¼Œshould_excludeåº”ä¸ºfalseï¼Œå¹¶åœ¨æ­¤å¤„è¯´æ˜'æ•°æ®ä¸å®Œæ•´ï¼Œå»ºè®®ç”¨æˆ·è‡ªè¡Œæ ¸å®ç›¸å…³è´¢åŠ¡æŠ¥è¡¨ã€‚'"
}
```
4. **çº¯å‡€è¾“å‡º**: ç¡®ä¿æœ€ç»ˆè¾“å‡ºæ˜¯ä¸€ä¸ªæ ¼å¼è‰¯å¥½çš„ã€å®Œæ•´çš„JSONæ•°ç»„ï¼Œå‰åä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡æœ¬æˆ–Markdownæ ‡è®°ã€‚
"""
    full_prompt = prompt_header + prompt_data_section + prompt_footer
    logging.info(f"æ„å»ºçš„æ‰¹é‡åˆ†æ Prompt æ€»é•¿åº¦ä¸º: {len(full_prompt)} å­—ç¬¦ã€‚")
    
    # ä¿å­˜å‘é€ç»™Geminiçš„å®Œæ•´å†…å®¹
    save_gemini_prompt_to_file(full_prompt, "stock_batch_analysis")
    
    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
        logging.info("æ­£åœ¨å‘ Gemini API å‘é€æ‰¹é‡åˆ†æè¯·æ±‚ (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
        start_time = time.time()
        
        # å¢åŠ 60ç§’ç­‰å¾…ï¼Œé˜²æ­¢APIé¢‘ç‡è¿‡å¿«
        logging.info("ç­‰å¾…60ç§’ï¼Œé˜²æ­¢APIé¢‘ç‡è¿‡å¿«...")
        time.sleep(60)
        
        response = model.generate_content(full_prompt)
        
        elapsed_time = time.time() - start_time
        logging.info(f"åœ¨ {elapsed_time:.2f} ç§’å†…æ”¶åˆ° Gemini API çš„å“åº”ã€‚")
        
        raw_response_text = response.text
        logging.info(f"æ”¶åˆ°æ¥è‡ªAIçš„åŸå§‹å“åº”æ–‡æœ¬ (å‰1000å­—ç¬¦): {raw_response_text[:1000]}...")

        response_text = raw_response_text.strip()
        match = re.search(r'```(json)?\s*\n(.*?)\n```', response_text, re.DOTALL)
        if match:
            json_text = match.group(2).strip()
            logging.info("å·²ä»Markdownä»£ç å—ä¸­æå–JSONå†…å®¹ã€‚")
        else:
            json_text = response_text
            logging.warning("å“åº”ä¸­æœªæ‰¾åˆ°Markdown JSONä»£ç å—ï¼Œå°†å°è¯•ç›´æ¥è§£ææ•´ä¸ªæ–‡æœ¬ã€‚")

        results_list = json.loads(json_text)
        if not isinstance(results_list, list):
            logging.error(f"å“åº”æ ¼å¼æ— æ•ˆ: æœŸæœ›æ˜¯åˆ—è¡¨ï¼Œä½†å¾—åˆ° {type(results_list)}")
            return {}
        
        # ç¡®ä¿æ¯ä¸ªç»“æœåŒ…å«å…¬å¸åç§°
        for result in results_list:
            if "name" not in result:
                for stock in all_stock_data:
                    if stock["code"] == result["code"]:
                        result["name"] = stock["name"]
                        break
        
        results_map = {str(result.get("code")): result for result in results_list if result.get("code")}
        
        if len(results_map) < len(all_stock_data):
            sent_codes = {s['code'] for s in all_stock_data}
            received_codes = set(results_map.keys())
            missing_codes = sent_codes - received_codes
            logging.warning(f"AIå“åº”ä¸­é—æ¼äº† {len(missing_codes)} æ”¯è‚¡ç¥¨çš„åˆ†æç»“æœã€‚é—æ¼çš„è‚¡ç¥¨ä»£ç : {missing_codes}")

        logging.info(f"æ‰¹é‡åˆ†æå®Œæˆã€‚æ”¶åˆ° {len(results_map)} æ”¯è‚¡ç¥¨çš„æœ‰æ•ˆåˆ†æç»“æœã€‚")
        return results_map
    except json.JSONDecodeError as e:
        logging.error(f"è§£æJSONå“åº”å¤±è´¥: {e}")
        logging.error(f"å¤±è´¥çš„å“åº”æ–‡æœ¬ (å‰1000å­—ç¬¦): {raw_response_text[:1000]}...")
        return {}
    except Exception as e:
        logging.error(f"æ‰¹é‡AIåˆ†ææœŸé—´å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        return {}

def save_stock_analysis_results(all_stock_data):
    """å°†æœ€ç»ˆçš„åˆ†æç»“æœä¿å­˜ä¸ºJSONå’ŒMarkdownæ–‡ä»¶ã€‚"""
    logging.info("æ­£åœ¨ä¿å­˜æ‰€æœ‰è‚¡ç¥¨ç­›é€‰åˆ†æç»“æœ...")
    current_time = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_data = [{"code": s["code"], "name": s["name"], "analysis_result": s.get("analysis_result", {})} for s in all_stock_data]
    
    json_path = os.path.join(ANALYSIS_RESULT_DIR, f"stock_analysis_{current_time}.json")
    md_path = os.path.join(ANALYSIS_RESULT_DIR, f"stock_analysis_{current_time}.md")

    logging.info(f"å‡†å¤‡å°†JSONç»“æœå†™å…¥: {json_path}")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)

    logging.info(f"å‡†å¤‡å°†MarkdownæŠ¥å‘Šå†™å…¥: {md_path}")
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(f"# è‚¡ç¥¨ç­›é€‰åˆ†ææŠ¥å‘Š (AI-Powered)\n\n**åˆ†ææ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        included = [s for s in report_data if not s.get('analysis_result', {}).get('should_exclude')]
        excluded = [s for s in report_data if s.get('analysis_result', {}).get('should_exclude')]
        f.write(f"## ç­›é€‰æ‘˜è¦\n\n- **æ€»åˆ†æè‚¡ç¥¨æ•°:** {len(report_data)}\n- **å»ºè®®ä¿ç•™:** {len(included)}\n- **å»ºè®®æ’é™¤:** {len(excluded)}\n\n")
        
        f.write("## å»ºè®®ä¿ç•™çš„è‚¡ç¥¨\n\n")
        if included:
            f.write("| è‚¡ç¥¨ä»£ç  | å…¬å¸åç§° | AIåˆ†ææ‘˜è¦ |\n|:---:|:---:|:---|\n")
            for stock in included:
                analysis_text = stock['analysis_result'].get('analysis', 'N/A')
                if "å»ºè®®ç”¨æˆ·è‡ªè¡Œæ ¸å®" in analysis_text:
                    analysis_text = f"**{analysis_text}**"
                f.write(f"| {stock['code']} | {stock['name']} | {analysis_text} |\n")
        else:
            f.write("æ— ã€‚\n\n")
            
        f.write("\n## å»ºè®®æ’é™¤çš„è‚¡ç¥¨\n\n")
        if excluded:
            f.write("| è‚¡ç¥¨ä»£ç  | å…¬å¸åç§° | æ’é™¤åŸå›  | AIåˆ†ææ‘˜è¦ |\n|:---:|:---:|:---:|:---|\n")
            for stock in excluded:
                reason = stock['analysis_result'].get('reason', 'N/A')
                analysis = stock['analysis_result'].get('analysis', 'N/A')
                f.write(f"| {stock['code']} | {stock['name']} | **{reason}** | {analysis} |\n")
        else:
            f.write("æ— ã€‚\n\n")
            
    logging.info(f"åˆ†æç»“æœå·²æˆåŠŸä¿å­˜è‡³ {json_path} å’Œ {md_path}")
    return md_path

def perform_stock_check_analysis(report_file_path):
    """æ‰§è¡Œè‚¡ç¥¨ç­›é€‰åˆ†æå¹¶å°†ç»“æœè¡¥å……åˆ°æŠ¥å‘Šæ–‡ä»¶"""
    logging.info("===== å¼€å§‹æ‰§è¡Œè‚¡ç¥¨ç­›é€‰å·¥ä½œæµ (CheckåŠŸèƒ½) =====")
    if not GEMINI_API_KEY:
        logging.error("Gemini API æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œè‚¡ç¥¨ç­›é€‰åˆ†æã€‚")
        return False

    # 1. æå–è‚¡ç¥¨
    stocks_to_analyze = extract_stock_recommendations(report_file_path)
    if not stocks_to_analyze:
        logging.error("æ— æ³•ä»æŠ¥å‘Šä¸­æå–åˆ°è‚¡ç¥¨åˆ—è¡¨ï¼ŒCheckåŠŸèƒ½ç»ˆæ­¢ã€‚")
        return False

    # 2. è·å–æ•°æ®
    logging.info("\n--- æ­¥éª¤ 1/4: ä» Yahoo Finance è·å–æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ® ---")
    all_stock_data = []
    for stock in tqdm(stocks_to_analyze, desc="è·å–é›…è™è´¢ç»æ•°æ®"):
        stock_data_package = fetch_stock_data_from_yahoo(stock)
        if stock_data_package:
            all_stock_data.append(stock_data_package)
        else:
            logging.warning(f"æ— æ³•è·å–è‚¡ç¥¨ {stock['code']}({stock['name']}) çš„æ•°æ®ï¼Œå°†åœ¨åˆ†æä¸­è·³è¿‡ã€‚")

    if not all_stock_data:
        logging.error("æœªèƒ½æˆåŠŸè·å–ä»»ä½•è‚¡ç¥¨çš„æ•°æ®ï¼ŒCheckåŠŸèƒ½ç»ˆæ­¢ã€‚")
        return False
    logging.info(f"æˆåŠŸè·å–äº† {len(all_stock_data)} / {len(stocks_to_analyze)} æ”¯è‚¡ç¥¨çš„æ•°æ®ã€‚")

    # 3. AIåˆ†æ
    logging.info("\n--- æ­¥éª¤ 2/4: å¼€å§‹ AI æ‰¹é‡åˆ†æ ---")
    analysis_results_map = analyze_stocks_in_batch(all_stock_data)
    if not analysis_results_map:
        logging.error("AIæ‰¹é‡åˆ†ææœªèƒ½è¿”å›æœ‰æ•ˆç»“æœï¼ŒCheckåŠŸèƒ½ç»ˆæ­¢ã€‚")
        return False
        
    error_result = {"should_exclude": True, "reason": "åˆ†æå¤±è´¥", "analysis": "æœªèƒ½ä»AIè·å–å¯¹æ­¤è‚¡ç¥¨çš„æœ‰æ•ˆåˆ†æã€‚è¯·æ£€æŸ¥æ—¥å¿—ä¸­AIçš„åŸå§‹å“åº”ã€‚"}
    for stock_data in all_stock_data:
        stock_code = stock_data['code']
        stock_data['analysis_result'] = analysis_results_map.get(stock_code, error_result)
    logging.info("å·²æˆåŠŸæ˜ å°„æ‰€æœ‰AIåˆ†æç»“æœã€‚")

    # 4. ä¿å­˜ç»“æœ
    logging.info("\n--- æ­¥éª¤ 3/4: ä¿å­˜è‚¡ç¥¨ç­›é€‰åˆ†ææŠ¥å‘Š ---")
    stock_analysis_report_path = save_stock_analysis_results(all_stock_data)
    if not stock_analysis_report_path:
        logging.error("ä¿å­˜è‚¡ç¥¨ç­›é€‰æŠ¥å‘Šå¤±è´¥ï¼ŒCheckåŠŸèƒ½ç»ˆæ­¢ã€‚")
        return False

    # 5. è¡¥å……åˆ°ä¸»æŠ¥å‘Š
    logging.info("\n--- æ­¥éª¤ 4/4: å°†è‚¡ç¥¨ç­›é€‰ç»“æœè¡¥å……åˆ°åŸå§‹æŠ¥å‘Š ---")
    if os.path.exists(stock_analysis_report_path):
        try:
            with open(stock_analysis_report_path, 'r', encoding='utf-8') as f:
                stock_analysis_content = f.read()
            with open(report_file_path, 'r', encoding='utf-8') as f:
                original_report = f.read()
            
            supplemented_report = original_report + "\n\n---\n\n" + stock_analysis_content
            
            with open(report_file_path, 'w', encoding='utf-8') as f:
                f.write(supplemented_report)
            logging.info(f"è‚¡ç¥¨ç­›é€‰åˆ†æç»“æœå·²æˆåŠŸè¡¥å……åˆ°åŸå§‹æŠ¥å‘Š: {report_file_path}")
            logging.info("===== è‚¡ç¥¨ç­›é€‰å·¥ä½œæµ (CheckåŠŸèƒ½) æˆåŠŸå®Œæˆ =====")
            return True
        except Exception as e:
            logging.error(f"è¡¥å……è‚¡ç¥¨ç­›é€‰åˆ†æç»“æœåˆ°åŸå§‹æŠ¥å‘Šå¤±è´¥: {e}")
            return False
    else:
        logging.error(f"æ‰¾ä¸åˆ°ç”Ÿæˆçš„è‚¡ç¥¨ç­›é€‰æŠ¥å‘Šæ–‡ä»¶: {stock_analysis_report_path}")
        return False

def process_all_data():
    """å¤„ç†æ‰€æœ‰æ•°æ®ï¼Œç”Ÿæˆã€ä¿å­˜å¹¶å‘é€ç»¼åˆåˆ†æ"""
    logging.info("=====================================")
    logging.info("========= å¼€å§‹æ–°ä¸€è½®æ•°æ®å¤„ç†æµç¨‹ =========")
    logging.info("=====================================")
    
    # æ­¥éª¤ 1: åŠ è½½æ‰€æœ‰åŸå§‹æ•°æ®
    logging.info("---------- æ­¥éª¤ 1/5: åŠ è½½æ‰€æœ‰åŸå§‹æ•°æ® ----------")
    reports_data = load_research_reports() or "æš‚æ— ç ”æŠ¥æ•°æ®"
    financial_news = load_financial_news()
    cls_news_raw = load_cls_news()
    _, market_analysis = load_market_data()
    market_analysis_str = market_analysis if market_analysis else "æš‚æ— å›½é™…å¸‚åœºåˆ†ææ•°æ®"

    # æ­¥éª¤ 2: ä½¿ç”¨AIæ‘˜è¦è´¢è”ç¤¾æ–°é—»
    logging.info("---------- æ­¥éª¤ 2/5: ä½¿ç”¨AIæ‘˜è¦è´¢è”ç¤¾æ–°é—» ----------")
    summarized_cls_news = summarize_cls_news_with_ai(cls_news_raw)
    # æ³¨æ„ï¼šå¦‚æœæ‘˜è¦å¤±è´¥ï¼Œå‡½æ•°ä¼šè¿”å›ä¸€æ¡é”™è¯¯ä¿¡æ¯ï¼Œè¿™æ¡ä¿¡æ¯ä¹Ÿä¼šè¢«åŒ…å«åœ¨æœ€ç»ˆçš„prompté‡Œï¼Œè®©ä¸»æ¨¡å‹çŸ¥æ™“æ­¤æƒ…å†µã€‚

    # æ­¥éª¤ 3: ç”Ÿæˆä¸»åˆ†ææŠ¥å‘Š
    logging.info("---------- æ­¥éª¤ 3/5: ç”Ÿæˆä¸»åˆ†ææŠ¥å‘Š ----------")
    analysis = generate_comprehensive_analysis(
        reports_data, 
        financial_news, 
        summarized_cls_news, 
        market_analysis_str
    )

    if analysis and "ç”Ÿæˆåˆ†æå¤±è´¥" not in analysis:
        logging.info("ä¸»åˆ†ææŠ¥å‘Šç”ŸæˆæˆåŠŸã€‚")
        # æ­¥éª¤ 4: ä¿å­˜æŠ¥å‘Šå¹¶æ‰§è¡ŒCheckåˆ†æ
        saved_file_path = save_analysis_to_file(analysis)
        if not saved_file_path:
            logging.error("ä¸»åˆ†ææŠ¥å‘Šä¿å­˜å¤±è´¥ï¼Œä½†ä»ä¼šå°è¯•æ¨é€ã€‚")
        
        if saved_file_path and os.path.exists(saved_file_path):
            logging.info("---------- æ­¥éª¤ 4/5: å¼€å§‹æ‰§è¡ŒCheckåˆ†ææµç¨‹ ----------")
            check_analysis_success = perform_stock_check_analysis(saved_file_path)
            if check_analysis_success:
                logging.info("Checkåˆ†ææµç¨‹æˆåŠŸå®Œæˆï¼ŒæŠ¥å‘Šå·²è¡¥å……ã€‚")
            else:
                logging.error("Checkåˆ†ææµç¨‹å¤±è´¥ï¼Œå°†æ¨é€æœªç»Checkåˆ†æè¡¥å……çš„æŠ¥å‘Šã€‚")
        else:
             logging.warning("ç”±äºä¸»æŠ¥å‘Šä¿å­˜å¤±è´¥æˆ–æœªæ‰¾åˆ°ï¼Œè·³è¿‡Checkåˆ†ææµç¨‹ã€‚")
             
        # æ­¥éª¤ 5: æ¨é€æœ€ç»ˆæŠ¥å‘Š
        logging.info("---------- æ­¥éª¤ 5/5: æ¨é€æœ€ç»ˆæŠ¥å‘Š ----------")
        final_analysis_content = analysis
        if saved_file_path and os.path.exists(saved_file_path):
            try:
                logging.info(f"æ­£åœ¨ä» {saved_file_path} è¯»å–æœ€ç»ˆæŠ¥å‘Šå†…å®¹è¿›è¡Œæ¨é€...")
                with open(saved_file_path, 'r', encoding='utf-8') as f:
                    final_analysis_content = f.read()
            except Exception as e:
                logging.error(f"è¯»å–è¡¥å……åçš„æŠ¥å‘Šå¤±è´¥: {e}ã€‚å°†æ¨é€åŸå§‹æŠ¥å‘Šã€‚")
        
        push_success = send_analysis_report(final_analysis_content)
        if push_success:
            logging.info("ç»¼åˆåˆ†ææŠ¥å‘Šå·²æˆåŠŸæ¨é€ã€‚")
        else:
            logging.error("ç»¼åˆåˆ†ææŠ¥å‘Šæ¨é€å¤±è´¥ã€‚")
    else:
        logging.error("æœªç”Ÿæˆæœ‰æ•ˆçš„ä¸»åˆ†ææŠ¥å‘Šï¼Œè·³è¿‡ä¿å­˜ã€Checkå’Œæ¨é€ã€‚")

    logging.info("=====================================")
    logging.info("========= æœ¬è½®æ•°æ®å¤„ç†æµç¨‹ç»“æŸ =========")
    logging.info("=====================================\n")


if __name__ == "__main__":
    process_all_data()
