#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import pandas as pd
import logging
import datetime
import json
import google.generativeai as genai
import time
from functools import wraps
from tqdm import tqdm
import yfinance as yf

# --- Configuration ---
# è®¾ç½®æ—¥å¿—è®°å½•ï¼Œä»¥æ˜¾ç¤ºæ—¶é—´æˆ³ã€æ—¥å¿—çº§åˆ«å’Œæ¶ˆæ¯ã€‚
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

# !!! é‡è¦ï¼šè¯·åœ¨æ­¤å¤„è®¾ç½®æ‚¨çš„ Gemini API å¯†é’¥ã€‚!!!
# å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ API å¯†é’¥ï¼Œè„šæœ¬å°†æ— æ³•å·¥ä½œã€‚
# æ‚¨å¯ä»¥å°†å…¶è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ GEMINI_API_KEYï¼Œæˆ–ç›´æ¥åœ¨ä¸‹é¢æ›¿æ¢ "YOUR_API_KEY_HERE"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "YOUR_API_KEY_HERE")

# è„šæœ¬è®¾è®¡ä¸ºå§‹ç»ˆä½¿ç”¨ AI åˆ†æã€‚
AI_ANALYSIS_ENABLED = True

if AI_ANALYSIS_ENABLED:
    if not GEMINI_API_KEY or GEMINI_API_KEY == "YOUR_API_KEY_HERE":
        logging.error("é”™è¯¯: Gemini API å¯†é’¥æœªè®¾ç½®ã€‚è¯·åœ¨è„šæœ¬ä¸­è®¾ç½® GEMINI_API_KEY æˆ–è®¾ç½®åŒåç¯å¢ƒå˜é‡ã€‚")
        AI_ANALYSIS_ENABLED = False
    else:
        try:
            genai.configure(api_key=GEMINI_API_KEY)
        except Exception as e:
            logging.error(f"Gemini API é…ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä½ çš„ API Key: {e}")
            AI_ANALYSIS_ENABLED = False

# --- ç›®å½•è®¾ç½® ---
# å®šä¹‰è¾“å…¥æŠ¥å‘Šã€è¾“å‡ºç»“æœå’ŒåŸå§‹æ•°æ®çš„ç›®å½•ã€‚
DAILY_REPORT_DIR = "æ¯æ—¥æŠ¥å‘Š"
ANALYSIS_RESULT_DIR = "åˆ†æç»“æœ"
RAW_DATA_DIR = "è‚¡ç¥¨åŸå§‹æ•°æ®" # ç”¨äºä¿å­˜é›…è™è´¢ç»åŸå§‹æ•°æ®çš„ç›®å½•
os.makedirs(ANALYSIS_RESULT_DIR, exist_ok=True)
os.makedirs(RAW_DATA_DIR, exist_ok=True)


# --- å·¥å…·å‡½æ•° ---
def retry(retries=3, delay=5, backoff=2):
    """
    ä¸€ä¸ªå¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•è£…é¥°å™¨ï¼Œç”¨äºç½‘ç»œè¯·æ±‚ã€‚
    è¿™æœ‰åŠ©äºå¤„ç†ä¸´æ—¶çš„ç½‘ç»œé—®é¢˜æˆ–APIé€Ÿç‡é™åˆ¶ã€‚
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            _retries, _delay = retries, delay
            while _retries > 0:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    _retries -= 1
                    if _retries == 0:
                        logging.warning(f"å‡½æ•° {func.__name__} åœ¨æ‰€æœ‰é‡è¯•åå¤±è´¥: {e}")
                        return None
                    logging.warning(f"å‡½æ•° {func.__name__} å¤±è´¥: {e}. å°†åœ¨ {_delay} ç§’åé‡è¯•... ({_retries} æ¬¡é‡è¯•å‰©ä½™)")
                    time.sleep(_delay)
                    _delay *= backoff
        return wrapper
    return decorator

# --- æŠ¥å‘Šè§£æä¸æ•°æ®è·å–æ¨¡å— ---
def get_latest_report():
    """æŸ¥æ‰¾å¹¶è¿”å›æœ€æ–°çš„æ¯æ—¥æŠ¥å‘Šçš„è·¯å¾„ã€‚"""
    logging.info("æ­£åœ¨æœç´¢æœ€æ–°çš„æ¯æ—¥æŠ¥å‘Š...")
    if not os.path.exists(DAILY_REPORT_DIR):
        logging.error(f"é”™è¯¯: æ¯æ—¥æŠ¥å‘Šç›®å½• '{DAILY_REPORT_DIR}' æœªæ‰¾åˆ°ã€‚")
        return None
    try:
        date_dirs = [d for d in os.listdir(DAILY_REPORT_DIR) if os.path.isdir(os.path.join(DAILY_REPORT_DIR, d)) and re.match(r'^\d{4}-\d{2}-\d{2}$', d)]
        if not date_dirs:
            logging.error(f"åœ¨ '{DAILY_REPORT_DIR}' ä¸­æœªæ‰¾åˆ°æ—¥æœŸæ ¼å¼ (YYYY-MM-DD) çš„å­ç›®å½•ã€‚")
            return None
        date_dirs.sort(key=lambda d: datetime.datetime.strptime(d, '%Y-%m-%d'), reverse=True)
        
        report_date_dir = os.path.join(DAILY_REPORT_DIR, date_dirs[0])
        report_files = [f for f in os.listdir(report_date_dir) if f.endswith('.md')]
        if not report_files:
            logging.error(f"åœ¨ç›®å½• '{report_date_dir}' ä¸­æœªæ‰¾åˆ° .md æ–‡ä»¶ã€‚")
            return None
        report_files.sort(reverse=True)
        
        latest_report = os.path.join(report_date_dir, report_files[0])
        logging.info(f"æˆåŠŸæ‰¾åˆ°æœ€æ–°æŠ¥å‘Š: {latest_report}")
        return latest_report
    except Exception as e:
        logging.error(f"æŸ¥æ‰¾æœ€æ–°æŠ¥å‘Šæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        return None

def extract_stock_recommendations(report_path):
    """ä» Markdown æŠ¥å‘Šä¸­è§£æå¹¶æå–æ¨èçš„è‚¡ç¥¨åˆ—è¡¨ (é‡‡ç”¨æ›´ç¨³å¥çš„é€è¡Œæ‰«æå®šä½æ³•)ã€‚"""
    logging.info(f"æ­£åœ¨ä»æŠ¥å‘Šä¸­æå–è‚¡ç¥¨æ¨è: {report_path}...")
    try:
        with open(report_path, 'r', encoding='utf-8-sig') as f:
            # é¢„å¤„ç†ï¼šæ›¿æ¢ä¸é—´æ–­ç©ºæ ¼å¹¶ç§»é™¤é¦–å°¾ç©ºæ ¼
            lines = [line.replace('\u00A0', ' ').strip() for line in f.readlines()]

        stocks = []
        table_start_index = -1

        # æ­¥éª¤ 1 & 2: å¯»æ‰¾è¡¨å¤´å’Œç´§éšå…¶åçš„åˆ†éš”çº¿
        for i, line in enumerate(lines):
            is_header = "è‚¡ç¥¨ä»£ç " in line and "å…¬å¸åç§°" in line and line.startswith('|')
            if is_header and i + 1 < len(lines):
                next_line = lines[i + 1]
                is_separator = re.match(r'^\s*\|(?:\s*:?---+:?\s*\|)+\s*$', next_line)
                if is_separator:
                    table_start_index = i + 2  # æ•°æ®ä»åˆ†éš”çº¿ä¹‹åå¼€å§‹
                    logging.info(f"åœ¨ç¬¬ {i} è¡Œæ‰¾åˆ°è¡¨æ ¼ï¼Œæ•°æ®ä»ç¬¬ {table_start_index} è¡Œå¼€å§‹ã€‚")
                    break
        
        if table_start_index == -1:
            logging.error("åœ¨æŠ¥å‘Šä¸­æ‰¾ä¸åˆ°æ ¼å¼æ­£ç¡®çš„è‚¡ç¥¨æ¨èè¡¨ï¼ˆç¼ºå°‘è¡¨å¤´æˆ–åˆ†éš”çº¿ï¼‰ã€‚")
            return None

        # æ­¥éª¤ 3: ä»æ‰¾åˆ°çš„èµ·å§‹ä½ç½®å¼€å§‹è§£ææ•°æ®è¡Œ
        for i in range(table_start_index, len(lines)):
            line = lines[i]
            if line.startswith('|') and line.endswith('|'):
                cells = [c.strip() for c in line.split('|')[1:-1]]
                
                if len(cells) >= 2:
                    stock_code = cells[0]
                    stock_name = cells[1]
                    
                    if re.match(r'^\d{6}$', stock_code):
                        # å…³é”®æ£€æŸ¥: ç¡®ä¿å…¬å¸åç§°ä¸ä¸ºç©º
                        if stock_name:
                            stocks.append({'code': stock_code, 'name': stock_name})
                        else:
                            logging.warning(f"è§£æåˆ°ç©ºçš„è‚¡ç¥¨åç§°ï¼Œä»£ç : {stock_code}ï¼Œè¡Œ: {line}")
                            # ä»ç„¶æ·»åŠ ï¼Œè®©åç»­æµç¨‹å°è¯•è¡¥å……
                            stocks.append({'code': stock_code, 'name': ''})
                    else:
                        logging.warning(f"è¡Œä¸­å‘ç°æ— æ•ˆè‚¡ç¥¨ä»£ç ï¼Œå‡å®šè¡¨æ ¼ç»“æŸ: {line}")
                        break
                else:
                    logging.warning(f"è·³è¿‡åˆ—æ•°ä¸è¶³çš„è¡Œ: {line}")
            else:
                logging.info(f"åœ¨ç¬¬ {i} è¡Œé‡åˆ°éè¡¨æ ¼æ ¼å¼è¡Œï¼Œåœæ­¢è§£æã€‚")
                break
        
        if not stocks:
            logging.error("è™½ç„¶æ‰¾åˆ°äº†è¡¨æ ¼ï¼Œä½†æœªèƒ½è§£æå‡ºä»»ä½•æœ‰æ•ˆçš„è‚¡ç¥¨æ•°æ®ã€‚")
            return None
            
        logging.info(f"æˆåŠŸæå– {len(stocks)} æ”¯æ¨èè‚¡ç¥¨ã€‚")
        return stocks

    except Exception as e:
        logging.error(f"æå–è‚¡ç¥¨æ¨èæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def get_yahoo_ticker_symbol(code):
    """ä¸ºé›…è™è´¢ç»è‡ªåŠ¨æ·»åŠ æ­£ç¡®çš„äº¤æ˜“æ‰€åç¼€ã€‚"""
    code_str = str(code).strip()
    if code_str.startswith(('60', '68')):
        return f"{code_str}.SS"
    elif code_str.startswith(('00', '30', '20')):
        return f"{code_str}.SZ"
    else:
        logging.warning(f"æ— æ³•ç¡®å®šè‚¡ç¥¨ä»£ç  {code_str} çš„äº¤æ˜“æ‰€ã€‚å°†é»˜è®¤å°è¯•ä¸Šäº¤æ‰€ (.SS)ã€‚")
        return f"{code_str}.SS"

@retry(retries=3, delay=10)
def fetch_stock_data_from_yahoo(stock_info):
    """ä»Yahoo Financeè·å–ä¸€æ”¯è‚¡ç¥¨çš„è‚¡ä»·å’Œè´¢åŠ¡æ•°æ®ï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ°ã€‚"""
    code = stock_info['code']
    name = stock_info['name']
    ticker_symbol = get_yahoo_ticker_symbol(code)
    stock_yf = yf.Ticker(ticker_symbol)

    # å¦‚æœä»æŠ¥å‘Šä¸­æå–çš„åç§°ä¸ºç©ºï¼Œåˆ™å°è¯•ä»é›…è™è´¢ç»è·å–ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
    if not name or not name.strip():
        logging.info(f"è‚¡ç¥¨ä»£ç  {code} çš„åç§°ä¸ºç©ºï¼Œæ­£åœ¨å°è¯•ä»é›…è™è´¢ç»è·å–...")
        try:
            stock_details = stock_yf.info
            updated_name = stock_details.get('longName') or stock_details.get('shortName')
            if updated_name:
                name = updated_name
                stock_info['name'] = updated_name
                logging.info(f"æˆåŠŸä¸ºä»£ç  {code} è·å–åˆ°åç§°: '{name}'")
            else:
                logging.warning(f"æ— æ³•ä»é›…è™è´¢ç»ä¸º {code} è·å–åˆ°åç§°ã€‚å°†ä½¿ç”¨ç©ºåç§°ã€‚")
        except Exception as e:
            logging.warning(f"å°è¯•ä»é›…è™è´¢ç»ä¸º {code} è·å–åç§°æ—¶å‡ºé”™: {e}")

    stock_data_package = {"code": code, "name": name}

    stock_raw_data_dir = os.path.join(RAW_DATA_DIR, code)
    os.makedirs(stock_raw_data_dir, exist_ok=True)

    hist_data = stock_yf.history(period="6mo")
    if hist_data.empty:
        logging.warning(f"æœªèƒ½è·å– {name}({code}) çš„è‚¡ä»·å†å²æ•°æ®ã€‚")
        return None
    
    hist_data.sort_index(ascending=False, inplace=True)
    hist_data.to_csv(os.path.join(stock_raw_data_dir, f"{code}_price_6mo.csv"))
    
    hist_data.index = hist_data.index.strftime('%Y-%m-%d')
    hist_data = hist_data.where(pd.notnull(hist_data), None)
    stock_data_package["price_data"] = hist_data.reset_index().to_dict('records')

    try:
        income_stmt = stock_yf.income_stmt.iloc[:, :4]
        balance_sheet = stock_yf.balance_sheet.iloc[:, :4]
        cash_flow = stock_yf.cashflow.iloc[:, :4]

        income_stmt.to_csv(os.path.join(stock_raw_data_dir, f"{code}_income_4y.csv"))
        balance_sheet.to_csv(os.path.join(stock_raw_data_dir, f"{code}_balance_4y.csv"))
        cash_flow.to_csv(os.path.join(stock_raw_data_dir, f"{code}_cashflow_4y.csv"))

        def format_financial_statement(df):
            df.columns = df.columns.strftime('%Y-%m-%d')
            transposed_df = df.transpose()
            transposed_df = transposed_df.where(pd.notnull(transposed_df), None)
            return transposed_df.reset_index().to_dict('records')

        stock_data_package["income_statement"] = format_financial_statement(income_stmt)
        stock_data_package["balance_sheet"] = format_financial_statement(balance_sheet)
        stock_data_package["cash_flow"] = format_financial_statement(cash_flow)
        
        logging.info(f"å·²ä¸ºè‚¡ç¥¨ {code} ä¿å­˜åŸå§‹æ•°æ®äº '{stock_raw_data_dir}'")
    except Exception as e:
        logging.warning(f"è·å– {name}({code}) çš„è´¢åŠ¡æ•°æ®æ—¶å‡ºé”™ (å¯èƒ½æ˜¯æ•°æ®ä¸å®Œæ•´): {e}ã€‚è´¢åŠ¡æ•°æ®å°†ä¸ºç©ºã€‚")
        stock_data_package["income_statement"] = []
        stock_data_package["balance_sheet"] = []
        stock_data_package["cash_flow"] = []

    return stock_data_package

# --- AI åˆ†æä¸ç»“æœä¿å­˜æ¨¡å— ---
@retry(retries=5, delay=10, backoff=2)
def analyze_stocks_in_batch(all_stock_data):
    """
    ä½¿ç”¨ Gemini AI æ¨¡å‹æ‰¹é‡åˆ†æè‚¡ç¥¨ã€‚
    ä¸€æ¬¡æ€§å‘é€æ‰€æœ‰è‚¡ç¥¨æ•°æ®ï¼Œå¹¶æœŸæœ›è·å¾—æ‰¹é‡å“åº”ã€‚
    """
    if not AI_ANALYSIS_ENABLED:
        logging.warning("AI_ANALYSIS_ENABLED ä¸º Falseï¼Œè·³è¿‡ AI åˆ†æã€‚")
        return {}

    logging.info(f"checkæ­£åœ¨ä½¿ç”¨ Gemini AI æ‰¹é‡åˆ†æ {len(all_stock_data)} æ”¯è‚¡ç¥¨...")

    prompt_header = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œã€ä¸€ä¸ä¸è‹Ÿçš„Aè‚¡è‚¡ç¥¨åˆ†æå¸ˆï¼Œæ“…é•¿é€šè¿‡è§£è¯»è‚¡ä»·èµ°åŠ¿å’Œè´¢åŠ¡æŠ¥è¡¨æ¥è¯†åˆ«é•¿æœŸä»·å€¼å’Œæ½œåœ¨é£é™©ã€‚

# ä»»åŠ¡
æ ¹æ®ä¸‹é¢æä¾›çš„ä¸€ç»„è‚¡ç¥¨åˆ—è¡¨åŠå…¶ä»Yahoo Financeè·å–çš„åŸå§‹æ•°æ®ï¼ˆè¿‘åŠå¹´è‚¡ä»·å†å²å’Œè¿‘å‡ å¹´å¹´åº¦è´¢åŠ¡æŠ¥è¡¨ï¼‰ï¼Œä½ å¿…é¡» **æ— ä¸€ä¾‹å¤–åœ°** å¯¹åˆ—è¡¨ä¸­çš„ **æ¯ä¸€æ”¯** è‚¡ç¥¨è¿›è¡Œç»¼åˆåˆ†æã€‚ç„¶åï¼Œæ ¹æ®ä»¥ä¸‹æ ¸å¿ƒæ ‡å‡†ï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥å°†å…¶ **æ’é™¤** åœ¨ä¸€ä¸ªè§‚å¯Ÿåˆ—è¡¨ä¸­ï¼š

1. **æ¶¨å¹…å·¨å¤§ï¼Œç‚’ä½œæ¥è¿‘æ³¡æ²«**: è‚¡ç¥¨ä»·æ ¼åœ¨çŸ­æœŸå†…ï¼ˆä¾‹å¦‚è¿‡å»åŠå¹´ï¼‰å·²ç»ç»å†äº†éå¸¸å·¨å¤§çš„æ¶¨å¹…ï¼Œå¹¶é€šè¿‡è€ƒè™‘æœªæ¥é¢„ä¼°å¸‚ç›ˆç‡ç­‰ä¼°å€¼ä½“ç³»ã€‚æ¥ç¡®å®šä¼°å€¼æ˜¯å¦è¿‡é«˜ï¼Œå­˜åœ¨æ³¡æ²«é£é™©ã€‚
2. **ä¸šç»©å‡ºç°è¿‡æ˜æ˜¾é—®é¢˜**: å…¬å¸è¿‘æœŸå†…çš„è´¢åŠ¡æŠ¥è¡¨å­˜åœ¨é£é™©ç‚¹ï¼šå¦‚æ”¶å…¥å’Œæ¯›åˆ©ç‡çš„å¤§å¹…ä¸‹æ»‘ï¼Œèµ„äº§è´Ÿå€ºæƒ…å†µå˜å·®ï¼Œç°é‡‘æµæ˜æ˜¾å‡å°‘ç­‰

# ç‰¹æ®Šæƒ…å†µå¤„ç†
- **æ•°æ®ä¸å®Œæ•´**: å¦‚æœæŸæ”¯è‚¡ç¥¨çš„å…³é”®è´¢åŠ¡æ•°æ®ï¼ˆå¦‚åˆ©æ¶¦è¡¨ã€ç°é‡‘æµé‡è¡¨ï¼‰ä¸¥é‡ç¼ºå¤±ï¼Œ**ä¸è¦å°†å…¶æ’é™¤**ã€‚åœ¨åˆ†æä¸­æ˜ç¡®æŒ‡å‡ºæ•°æ®ä¸å®Œæ•´ï¼Œå¹¶å»ºè®®ç”¨æˆ·è‡ªè¡Œæ ¸å®ã€‚
"""

    prompt_data_section = f"""
# å¾…åˆ†ææ•°æ®
è¿™æ˜¯å¾…åˆ†æçš„è‚¡ç¥¨åˆ—è¡¨åŠå…¶æ•°æ®ï¼š
```json
{json.dumps(all_stock_data, ensure_ascii=False, indent=2, default=str)}
```
"""
    
    prompt_footer = """
# åˆ†æä¸è¾“å‡ºè¦æ±‚
1.  **ä¸¥æ ¼éµå®ˆ**: ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ’é™¤æ ‡å‡†å’Œç‰¹æ®Šæƒ…å†µå¤„ç†è§„åˆ™ï¼Œå¯¹è¾“å…¥æ•°æ®ä¸­çš„ **æ¯ä¸€æ”¯è‚¡ç¥¨** è¿›è¡Œåˆ†æå¹¶å½¢æˆä¸€ä¸ªJSONå¯¹è±¡ã€‚
2.  **å®Œæ•´æ€§ä¿è¯**: è¿”å›çš„JSONæ•°ç»„ **å¿…é¡»** åŒ…å«ä¸è¾“å…¥æ•°æ®ä¸­ç›¸åŒæ•°é‡çš„å¯¹è±¡ã€‚**ç»å¯¹ä¸è¦é—æ¼ä»»ä½•ä¸€æ”¯è‚¡ç¥¨**ã€‚
3.  **å¼ºåˆ¶æ ¼å¼**: æ•°ç»„ä¸­æ¯ä¸ªå¯¹è±¡çš„ç»“æ„ **å¿…é¡»** å¦‚ä¸‹æ‰€ç¤ºã€‚`code` å’Œ `name` å­—æ®µè‡³å…³é‡è¦ã€‚
```json
{
  "code": "åŸå§‹è‚¡ç¥¨ä»£ç , ä¾‹å¦‚ '600519'",
  "name": "å…¬å¸åç§°, ä¾‹å¦‚ 'è´µå·èŒ…å°'",
  "should_exclude": boolean,
  "reason": "ä»…åœ¨ should_exclude ä¸º true æ—¶å¡«å†™æ­¤å­—æ®µï¼Œä»'è¿‘æœŸæ˜æ˜¾ä¸‹è·Œèµ°åŠ¿'ã€'æ¶¨å¹…å·¨å¤§æ¥è¿‘æ³¡æ²«'ã€'è´¢åŠ¡æœ‰ä¸¥é‡é—®é¢˜'ä¸­é€‰æ‹©ä¸€ä¸ªã€‚å¦‚æœshould_excludeä¸ºfalseï¼Œæ­¤å­—æ®µåº”ä¸ºç©ºå­—ç¬¦ä¸²æˆ–nullã€‚",
  "analysis": "æä¾›ä¸€å¥è¯çš„ç®€æ˜æ‰¼è¦çš„åˆ†æï¼Œè§£é‡Šä½ åšå‡ºè¯¥åˆ¤æ–­çš„æ ¸å¿ƒä¾æ®ã€‚å¯¹äºæ•°æ®ä¸å®Œæ•´çš„è‚¡ç¥¨ï¼Œshould_excludeåº”ä¸ºfalseï¼Œå¹¶åœ¨æ­¤å¤„è¯´æ˜â€˜æ•°æ®ä¸å®Œæ•´ï¼Œå»ºè®®ç”¨æˆ·è‡ªè¡Œæ ¸å®ç›¸å…³è´¢åŠ¡æŠ¥è¡¨ã€‚â€™"
}
```
4.  **çº¯å‡€è¾“å‡º**: ç¡®ä¿æœ€ç»ˆè¾“å‡ºæ˜¯ä¸€ä¸ªæ ¼å¼è‰¯å¥½çš„ã€å®Œæ•´çš„JSONæ•°ç»„ï¼Œå‰åä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡æœ¬æˆ–Markdownæ ‡è®°ã€‚
"""

    full_prompt = prompt_header + prompt_data_section + prompt_footer

    try:
        model = genai.GenerativeModel('gemini-2.5-pro')
        
        logging.info("æ­£åœ¨å‘ Gemini API å‘é€è¯·æ±‚ (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
        start_time = time.time()
        response = model.generate_content(full_prompt)
        elapsed_time = time.time() - start_time
        logging.info(f"åœ¨ {elapsed_time:.2f} ç§’å†…æ”¶åˆ° Gemini API çš„å“åº”")
        
        logging.debug(f"æ”¶åˆ°æ¥è‡ªAIçš„åŸå§‹å“åº”æ–‡æœ¬: {response.text[:1000]}...")

        response_text = response.text.strip()
        match = re.search(r'```(json)?\n(.*)\n```', response_text, re.DOTALL)
        if match:
            response_text = match.group(2).strip()
        
        results_list = json.loads(response_text)
        
        if not isinstance(results_list, list):
            logging.error(f"å“åº”æ ¼å¼æ— æ•ˆ: æœŸæœ›æ˜¯åˆ—è¡¨ï¼Œä½†å¾—åˆ° {type(results_list)}")
            return {}
            
        results_map = {str(result.get("code")): result for result in results_list if result.get("code")}
        
        if len(results_map) < len(all_stock_data):
            logging.warning(f"AIå“åº”ä¸­é—æ¼äº† {len(all_stock_data) - len(results_map)} æ”¯è‚¡ç¥¨çš„åˆ†æç»“æœã€‚")

        logging.info(f"æ‰¹é‡åˆ†æå®Œæˆã€‚æ”¶åˆ° {len(results_map)} æ”¯è‚¡ç¥¨çš„åˆ†æç»“æœã€‚")
        return results_map
        
    except json.JSONDecodeError as e:
        logging.error(f"è§£æJSONå“åº”å¤±è´¥: {e}")
        logging.error(f"å¤±è´¥çš„å“åº”æ–‡æœ¬ (å‰500å­—ç¬¦): {response.text[:500]}...")
        return {}
    except Exception as e:
        logging.error(f"æ‰¹é‡AIåˆ†ææœŸé—´å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        return {}

def save_analysis_results(all_stock_data):
    """å°†æœ€ç»ˆçš„åˆ†æç»“æœä¿å­˜ä¸ºJSONå’ŒMarkdownæ–‡ä»¶ã€‚"""
    logging.info("æ­£åœ¨ä¿å­˜æ‰€æœ‰åˆ†æç»“æœ...")
    current_time = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    report_data = [{"code": s["code"], "name": s["name"], "analysis_result": s.get("analysis_result", {})} for s in all_stock_data]
    
    json_path = os.path.join(ANALYSIS_RESULT_DIR, f"stock_analysis_{current_time}.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    md_path = os.path.join(ANALYSIS_RESULT_DIR, f"stock_analysis_{current_time}.md")
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(f"# è‚¡ç¥¨ç­›é€‰åˆ†ææŠ¥å‘Š (AI-Powered)\n\n**åˆ†ææ—¶é—´:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        included = [s for s in report_data if not s.get('analysis_result', {}).get('should_exclude')]
        excluded = [s for s in report_data if s.get('analysis_result', {}).get('should_exclude')]
        
        f.write(f"## ç­›é€‰æ‘˜è¦\n\n- **æ€»åˆ†æè‚¡ç¥¨æ•°:** {len(report_data)}\n- **å»ºè®®ä¿ç•™:** {len(included)}\n- **å»ºè®®æ’é™¤:** {len(excluded)}\n\n")
        
        f.write("## å»ºè®®ä¿ç•™çš„è‚¡ç¥¨\n\n")
        if included:
            f.write("| è‚¡ç¥¨ä»£ç  | å…¬å¸åç§° | AIåˆ†ææ‘˜è¦ |\n|:---:|:---:|:---|\n")
            for stock in included:
                analysis_text = stock['analysis_result'].get('analysis', 'N/A')
                if "å»ºè®®ç”¨æˆ·è‡ªè¡Œæ ¸å®" in analysis_text:
                    analysis_text = f"**{analysis_text}**"
                f.write(f"| {stock['code']} | {stock['name']} | {analysis_text} |\n")
        else:
            f.write("æ— ã€‚\n\n")
        
        f.write("\n## å»ºè®®æ’é™¤çš„è‚¡ç¥¨\n\n")
        if excluded:
            f.write("| è‚¡ç¥¨ä»£ç  | å…¬å¸åç§° | æ’é™¤åŸå›  | AIåˆ†ææ‘˜è¦ |\n|:---:|:---:|:---:|:---|\n")
            for stock in excluded:
                f.write(f"| {stock['code']} | {stock['name']} | **{stock['analysis_result'].get('reason', 'N/A')}** | {stock['analysis_result'].get('analysis', 'N/A')} |\n")
        else:
            f.write("æ— ã€‚\n\n")
    
    logging.info(f"åˆ†æç»“æœå·²ä¿å­˜è‡³ {json_path} å’Œ {md_path}")
    return md_path

# --- ä¸»å‡½æ•° ---
def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºç¼–æ’æ•´ä¸ªåˆ†æå·¥ä½œæµã€‚"""
    logging.info("===== å¼€å§‹è‚¡ç¥¨ç­›é€‰å·¥ä½œæµ (Yahoo Finance + AIæ‰¹é‡åˆ†ææ¨¡å¼) =====")
    
    if not AI_ANALYSIS_ENABLED:
        logging.error("Gemini AI æœªå¯ç”¨æˆ–é…ç½®å¤±è´¥ï¼Œæ— æ³•æ‰§è¡Œåˆ†æã€‚ç¨‹åºé€€å‡ºã€‚")
        return

    report_path = get_latest_report()
    if not report_path:
        logging.error("æœªæ‰¾åˆ°å¯ç”¨çš„æ¯æ—¥æŠ¥å‘Šï¼Œç¨‹åºé€€å‡ºã€‚")
        return
    
    stocks_to_analyze = extract_stock_recommendations(report_path)
    if not stocks_to_analyze:
        logging.error("æ— æ³•ä»æŠ¥å‘Šä¸­æå–è‚¡ç¥¨åˆ—è¡¨ï¼Œç¨‹åºé€€å‡ºã€‚")
        return
    
    logging.info("\n--- æ­¥éª¤ 1: ä» Yahoo Finance è·å–æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ® ---")
    all_stock_data = []
    
    for stock in tqdm(stocks_to_analyze, desc="è·å–å¹¶ä¿å­˜é›…è™è´¢ç»æ•°æ®"):
        stock_data_package = fetch_stock_data_from_yahoo(stock)
        if stock_data_package:
            all_stock_data.append(stock_data_package)
        else:
            logging.warning(f"æ— æ³•è·å–è‚¡ç¥¨ {stock['code']} çš„æ•°æ®ï¼Œå°†åœ¨åˆ†æä¸­è·³è¿‡ã€‚")

    if not all_stock_data:
        logging.error("æœªèƒ½æˆåŠŸè·å–ä»»ä½•è‚¡ç¥¨çš„æ•°æ®ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    logging.info(f"æˆåŠŸè·å–äº† {len(all_stock_data)} / {len(stocks_to_analyze)} æ”¯è‚¡ç¥¨çš„æ•°æ®ã€‚")
    
    logging.info("\n--- æ­¥éª¤ 2: å¼€å§‹ AI æ‰¹é‡åˆ†æ ---")
    
    analysis_results_map = analyze_stocks_in_batch(all_stock_data)
    
    error_result = {"should_exclude": True, "reason": "åˆ†æå¤±è´¥", "analysis": "æœªèƒ½ä»AIè·å–å¯¹æ­¤è‚¡ç¥¨çš„æœ‰æ•ˆåˆ†æã€‚æ£€æŸ¥æ—¥å¿—ä¸­AIçš„åŸå§‹å“åº”ã€‚"}
    
    # å°†åˆ†æç»“æœæ˜ å°„å›åŸå§‹æ•°æ®åˆ—è¡¨
    for stock_data in all_stock_data:
        stock_code = stock_data['code']
        ai_result = analysis_results_map.get(stock_code, error_result)
        stock_data['analysis_result'] = ai_result

        # --- BUG FIX ---
        # å¦‚æœåœ¨åˆå§‹è§£æå’ŒYahooæŸ¥è¯¢ååç§°ä»ç„¶ç¼ºå¤±ï¼Œåˆ™å°è¯•ä»AIçš„å“åº”ä¸­æ¢å¤åç§°ã€‚
        # è¿™å¢åŠ äº†è„šæœ¬çš„å¥å£®æ€§ï¼Œä»¥åº”å¯¹è§£æå¤±è´¥æˆ–æ•°æ®æºé—®é¢˜ã€‚
        current_name = stock_data.get("name", "").strip()
        ai_name = ai_result.get("name", "").strip()
        if not current_name and ai_name:
            stock_data["name"] = ai_name
            logging.info(f"ä¸ºä»£ç  {stock_code} ä» AI å“åº”ä¸­æ¢å¤äº†åç§°: '{ai_name}'")

    logging.info("å·²æˆåŠŸæ˜ å°„æ‰€æœ‰AIåˆ†æç»“æœã€‚")
    
    logging.info("\n--- æ­¥éª¤ 3: ä¿å­˜æœ€ç»ˆåˆ†ææŠ¥å‘Š ---")
    final_report_path = save_analysis_results(all_stock_data)
    
    logging.info("===== è‚¡ç¥¨ç­›é€‰å·¥ä½œæµæˆåŠŸå®Œæˆ =====")
    if final_report_path:
        print("\n" + "="*60)
        print("ğŸ‰ åˆ†ææŠ¥å‘Šå·²æˆåŠŸç”Ÿæˆï¼")
        print(f"   Markdown æ–‡ä»¶å: {os.path.basename(final_report_path)}")
        print(f"   å®Œæ•´è·¯å¾„: {final_report_path}")
        print("="*60)

if __name__ == "__main__":
    main()

